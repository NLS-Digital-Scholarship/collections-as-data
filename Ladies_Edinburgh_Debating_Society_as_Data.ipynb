{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ladies' Edinburgh Debating Society as Data\n",
    "Created August-September 2020 for the National Library of Scotland's Data Foundry by Lucy Havens, Digital Library Research Intern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the *Ladies' Edinburgh Debating Society* Dataset\n",
    "The Ladies' Edinburgh Debating Society (LEDS) was founded by women in 1865 who were members of the upper-middle and higher classes at a time when women had limited higher education opportunities.  Members went on to play significant roles in education, suffrage, philanthropy, and anti-slavery efforts.  The LEDS Dataset contains digitized text from all volumes of two journals the Society published: \"The Attempt\" and \"The Ladies' Edinburgh Magazine.\"  The first journal contains 10 volumes published from 1865 through 1874.  The second journal contains six volumes published from 1875 through 1880.  \n",
    "\n",
    "The Ladies' Edinburgh Debating Society, also known as the Edinburgh Essay Society and the Ladies' Edinburgh Essay Society, was dissolved in 1935.  A year later, in 1936, the National Library of Scotland acquired the volumes that were digitized in this dataset.\n",
    "* Data format: digitised text\n",
    "* Data creation process: Optical Character Recognition (OCR)\n",
    "* Data source: https://data.nls.uk/data/digitised-collections/edinburgh-ladies-debating-society/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preparation\n",
    "Import libraries to use for cleaning, summarizing and exploring the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# To prevent SSL certificate failure\n",
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "    getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Libraries for data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Libraries for visualization\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Libraries for text analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('tagsets')  # part of speech tags\n",
    "from nltk.draw.dispersion import dispersion_plot as displt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nls-text-ladiesDebating folder (downloadable as *Just the text* data from the website at the top of this notebook) contains TXT files of digitized text, with numerical names, as well as a CSV inventory file and a TXT ReadMe file.  Load only the TXT files of digitized text and **tokenize** the text (which splits a string into separate words and punctuation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â', '\\x80¢*', 'â', '\\x80¢', 'UL', '.', 'u', '^\\\\,', 'THE', 'ATTEMPT']\n"
     ]
    }
   ],
   "source": [
    "corpus_folder = 'data/nls-text-ladiesDebating/'\n",
    "wordlists = PlaintextCorpusReader(corpus_folder, '\\d.*', encoding='latin1')\n",
    "corpus_tokens = wordlists.words()\n",
    "print(corpus_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you'd like to see how to specify a single TXT file to load as data, check out the Jupyter Notebook for the Britain and UK Handbooks!*\n",
    "\n",
    "It's hard to get a sense of how accurately the text has been digitized from this list of 10 tokens, so let's look at one of these words in context.  To see phrases in which \"Edinburgh\" is used, we can use the concordance() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 2079 matches:\n",
      "UM MELIORIS MVl .\" FEINTED FOR THE EDINBURGH ESSAY SOCIETY . EEID & SON , SHORE\n",
      "atriculated into the University of Edinburgh , where he graduated in Arts , aft\n",
      "pany has been making a stir in the Edinburgh world ), he was making a long spee\n",
      " May proÂ ¬ verbially favours us , Edinburgh holds May one of the dearest of th\n",
      "on of Ecclesiastical Courts met in Edinburgh this month are no less noteworthy \n",
      "wning glory of the month of May in Edinburgh , supplying plenty of gaiety and g\n",
      "go and find out .\" Tlie hair of an Edinburgh cab - owner would stand on end did\n",
      "onal Gallery . Possibly some of my Edinburgh friends may not have had the oppor\n",
      "NE CONDUCTED BY THE MEMBERS OF THE EDINBURGH ESSAY SOCIETY . VOLUME III . \" AUS\n",
      "M MELIORIS uEVI .\" PRINTED FOR THE EDINBURGH ESSAY SOCIETY . COLSTON & SON , ED\n"
     ]
    }
   ],
   "source": [
    "t = Text(corpus_tokens)\n",
    "t.concordance('Edinburgh', lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has not been manually cleaned after OCR digitized text from \"The Attempt\" and \"The Ladies' Edinburgh Magazine\" so it's not surprising to see some non-words appear in the concordance.\n",
    "\n",
    "Before we do much analysis, let's get a sense of how much data we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total...\n",
      "  Characters in LEDS Data: 15096132\n",
      "  Words in LEDS Data: 3145535\n",
      "  Sentences in LEDS Data: 108011\n",
      "  Files in LEDS Data: 16\n"
     ]
    }
   ],
   "source": [
    "def corpusStatistics(plaintext_corpus_read_lists):\n",
    "    total_chars = 0\n",
    "    total_words = 0\n",
    "    total_sents = 0\n",
    "    total_files = 0\n",
    "    \n",
    "    # fileids are the TXT file names in the nls-text-ladies-Debating folder:\n",
    "    for fileid in plaintext_corpus_read_lists.fileids():\n",
    "        total_chars += len(plaintext_corpus_read_lists.raw(fileid))\n",
    "        total_words += len(plaintext_corpus_read_lists.words(fileid))\n",
    "        total_sents += len(plaintext_corpus_read_lists.sents(fileid))\n",
    "        total_files += 1\n",
    "    \n",
    "    print(\"Total...\")\n",
    "    print(\"  Characters in LEDS Data:\", total_chars)\n",
    "    print(\"  Words in LEDS Data:\", total_words)\n",
    "    print(\"  Sentences in LEDS Data:\", total_sents)\n",
    "    print(\"  Files in LEDS Data:\", total_files)\n",
    "\n",
    "corpusStatistics(wordlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create two subsets of the data, one for each journal.  To do so we first need to load the inventory (CSV file) that lists which file name corresponds with which journal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109857781.txt</td>\n",
       "      <td>Attempt - Volume 1 and Select writings - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103655648.txt</td>\n",
       "      <td>Attempt - Volume 2 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103655649.txt</td>\n",
       "      <td>Attempt - Volume 3 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103655650.txt</td>\n",
       "      <td>Attempt - Volume 4 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103655651.txt</td>\n",
       "      <td>Attempt - Volume 5 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>103655652.txt</td>\n",
       "      <td>Attempt - Volume 6 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>103655653.txt</td>\n",
       "      <td>Attempt - Volume 7 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>103655654.txt</td>\n",
       "      <td>Attempt - Volume 8 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>103655655.txt</td>\n",
       "      <td>Attempt - Volume 9 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>103655656.txt</td>\n",
       "      <td>Attempt - Volume 10 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>103655658.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 1 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>103655659.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 2 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>103655660.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 3 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>103655661.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 4 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>103655662.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 5 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>103655663.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 6 - U.393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fileid                                           title\n",
       "0   109857781.txt  Attempt - Volume 1 and Select writings - U.431\n",
       "1   103655648.txt                      Attempt - Volume 2 - U.431\n",
       "2   103655649.txt                      Attempt - Volume 3 - U.431\n",
       "3   103655650.txt                      Attempt - Volume 4 - U.431\n",
       "4   103655651.txt                      Attempt - Volume 5 - U.431\n",
       "5   103655652.txt                      Attempt - Volume 6 - U.431\n",
       "6   103655653.txt                      Attempt - Volume 7 - U.431\n",
       "7   103655654.txt                      Attempt - Volume 8 - U.431\n",
       "8   103655655.txt                      Attempt - Volume 9 - U.431\n",
       "9   103655656.txt                     Attempt - Volume 10 - U.431\n",
       "10  103655658.txt   Ladies' Edinburgh Magazine - Volume 1 - U.393\n",
       "11  103655659.txt   Ladies' Edinburgh Magazine - Volume 2 - U.393\n",
       "12  103655660.txt   Ladies' Edinburgh Magazine - Volume 3 - U.393\n",
       "13  103655661.txt   Ladies' Edinburgh Magazine - Volume 4 - U.393\n",
       "14  103655662.txt   Ladies' Edinburgh Magazine - Volume 5 - U.393\n",
       "15  103655663.txt   Ladies' Edinburgh Magazine - Volume 6 - U.393"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/nls-text-ladiesDebating/ladiesDebating-inventory.csv', header=None, names=['fileid', 'title'])\n",
    "# Since we only have 16 files, we'll print the entire dataframe.  With larger dataframes\n",
    "# you may wish to use  df.head() or df.tail() to print only the first 5 or last 5 rows\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a two dictionaries of fileids and their associated journal titles, one for The Attempt and one for The Ladies' Edinburgh Magazine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'109857781.txt': 'Attempt - Volume 1 and Select writings - U.431', '103655648.txt': 'Attempt - Volume 2 - U.431', '103655649.txt': 'Attempt - Volume 3 - U.431', '103655650.txt': 'Attempt - Volume 4 - U.431', '103655651.txt': 'Attempt - Volume 5 - U.431', '103655652.txt': 'Attempt - Volume 6 - U.431', '103655653.txt': 'Attempt - Volume 7 - U.431', '103655654.txt': 'Attempt - Volume 8 - U.431', '103655655.txt': 'Attempt - Volume 9 - U.431', '103655656.txt': 'Attempt - Volume 10 - U.431'}\n",
      "{'103655658.txt': \"Ladies' Edinburgh Magazine - Volume 1 - U.393\", '103655659.txt': \"Ladies' Edinburgh Magazine - Volume 2 - U.393\", '103655660.txt': \"Ladies' Edinburgh Magazine - Volume 3 - U.393\", '103655661.txt': \"Ladies' Edinburgh Magazine - Volume 4 - U.393\", '103655662.txt': \"Ladies' Edinburgh Magazine - Volume 5 - U.393\", '103655663.txt': \"Ladies' Edinburgh Magazine - Volume 6 - U.393\"}\n"
     ]
    }
   ],
   "source": [
    "attempts = {}\n",
    "mags = {}\n",
    "for index, row in df.iterrows():\n",
    "    fileid = row['fileid']\n",
    "    title = row['title']\n",
    "    if 'Attempt' in title:\n",
    "        attempts[fileid] = title\n",
    "    else: # if 'Magazine' in title:\n",
    "        mags[fileid] = title\n",
    "\n",
    "print(attempts)\n",
    "print(mags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenient reference of only fileids, we can also create lists from the dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['103655658.txt', '103655659.txt', '103655660.txt', '103655661.txt', '103655662.txt', '103655663.txt']\n"
     ]
    }
   ],
   "source": [
    "attempt_ids = list(attempts.keys())\n",
    "mag_ids = list(mags.keys())\n",
    "print(mag_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Code cells in this section will have one function each, preceded by comments as markdown above the cell to narrate the cleaning process]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summary Statistics\n",
    "[Code cells in this section will have one function each, preceded with comments in a markdown cell narrating the summarization process]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Frequencies and Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Uniqueness and Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here - most frequent words, sentence structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Analysis (this section will be included for 2-3 datasets)\n",
    "[Code cells in this section will have one function each, preceded with comments in a markdown cell posing an exploratory research question]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 [exploratory research question 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 [exploratory research question 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations go here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
