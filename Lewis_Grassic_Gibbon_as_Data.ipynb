{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lewis Grassic Gibbon First Editions as Data\n",
    "Created August-September 2020 for the National Library of Scotland's Data Foundry by Lucy Havens, Digital Library Research Intern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Lewis Grassic Gibbon First Editions Dataset\n",
    "Lewis Grassic Gibbon is an early 20th century Scottish novelist who also published under his birth name, James Leslie Mitchell.  He was an  prolific writer for the short period of time (5 years) that he published fiction and non-fiction, and the NLS collection contains first editions of all his published books.  Gibbon's stories often featured strong central female characters, unusual for an early 20th century writer.  Gibbon's literary influence continues to be felt today: his book A Sunset Song was voted Scotland's favorite novel in 2016, and contemporary Scottish writers such as Ali Smith and E.L. Kennedy have noted Gibbon's influence on their own writing.\n",
    "\n",
    "* Data format: digitised text\n",
    "* Data creation process: Optical Character Recognition (OCR)\n",
    "* Data source: https://data.nls.uk/data/digitised-collections/lewis-grassic-gibbon-first-editions/\n",
    "***\n",
    "### Table of Contents\n",
    "0. [Preparation](#0.-Preparation)\n",
    "1. [Data Cleaning and Standardisation](#1.-Data-Cleaning-and-Standardisation)\n",
    "2. [Summary Statistics](#2.-Summary-Statistics)\n",
    "3. [Exploratory Analysis](#3.-Exploratory-Analysis)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preparation\n",
    "Import libraries to use for cleaning, summarising and exploring the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# To prevent SSL certificate failure\n",
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "    getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Libraries for data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Libraries for visualization\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Libraries for text analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('tagsets')  # part of speech tags\n",
    "from nltk.draw.dispersion import dispersion_plot as displt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nls-text-gibbon folder (downloadable as *Just the text* data from the website at the top of this notebook) contains TXT files of digitized text, with numerical names, as well as a CSV inventory file and a TXT ReadMe file.  Load only the TXT files of digitized text and **tokenise** the text (which splits the text into a list of its individual words and punctuation in the order they appear):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BY', 'ROBERT', 'MACLEHOSE', 'AND', 'CO', '.', 'LTD', '.', 'THE', 'UNIVERSITY', 'PRESS', ',', 'GLASGOW', 'ALL', 'RIGHTS']\n"
     ]
    }
   ],
   "source": [
    "corpus_folder = 'data/nls-text-gibbon/'\n",
    "wordlists = PlaintextCorpusReader(corpus_folder, '\\d.*', encoding='latin1')\n",
    "corpus_tokens = wordlists.words()\n",
    "print(corpus_tokens[100:115])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you'd like to see how to specify a single TXT file to load as data, check out the Jupyter Notebook for the Britain and UK Handbooks!*\n",
    "\n",
    "It's hard to get a sense of how accurately the text has been digitised from this list of 15 tokens, so let's look at one of these words in context.  To see phrases in which \"Scots\" is used, we can use the concordance() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 72 matches:\n",
      "UTHOR Novels forming the trilogy , A Scots Quair Part I . Sunset Song ( 1932 ) \n",
      "igins of that son . He had the usual Scots passion for educationâ  that passi\n",
      "Â ¬ thing of the same quality , this Scots desire and pride in education , that\n",
      "hood . Because second - sight in the Scots was a fiction not yet invented in th\n",
      "us these were the lesser gods of the Scots pantheon , witches and wizards and k\n",
      " the ambition and intention of every Scots farmer to produce at least one son w\n",
      "ad litde relevance to religion . The Scots are not a religious people : long be\n",
      "is the function and privilege of the Scots minister : he is less a priest than \n",
      "e tenebrous imaginings of fear . The Scots ballads could have widened his geogr\n",
      " and climes , where beasts were un - Scots and untamed , and in blue waters str\n",
      "racteristic of one great division of Scots as Burns was of another . He saw the\n",
      "scular , with the solid frame of the Scots peasant redeemed of its usual squatn\n",
      " appreciation of 54 such niceties in Scots law . They blandly retorted that it \n",
      "corn . Hearing of this , Mungo , his Scots sentiment not yet withered , desired\n",
      "ew the splendours of summer from the Scots earth and flung them , palely colour\n",
      "ung them , palely colourful , into a Scots sky , he found perhaps the first tru\n",
      "e are many such curious words in the Scots dialects , pathetic reminders of the\n",
      "of gawkish puns ), and Mary Queen of Scots , whose blood was lapped by a dog . \n",
      "hy was no dull subject , and , being Scots , they found English fascinating eno\n",
      "ken to . . . . Speak English , never Scots . . . . Keep your mouth closed ( all\n",
      "e unattractive set of kings than the Scots from Bruce to James VI . They seemed\n",
      "vative sheet , a kind of ineffective Scots Morning Post . Northcliffism had har\n",
      " only Miss Domina . â  The Lowland Scots of Chapel oâ  Seddel , like that o\n",
      "eep , I found greater kinship , that Scots - descended Russian Learmont who som\n",
      " into submission . Then revolt among Scots , Australians , days when the little\n"
     ]
    }
   ],
   "source": [
    "t = Text(corpus_tokens)\n",
    "t.concordance(\"Scots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some mistakes but not too many!\n",
    "\n",
    "Note how NLTK's ``concordance`` method works: the word \"Scots\" appears with different meanings, sometimes referring to the language, other times referring to the people.  NLTK has a tagging method that identifies the parts of speech in sentences, so if we wanted to focus on the language Scots, we could look for instances of Scots being used as a noun.  If we wanted to focus on the people Scots, we could look for instances of Scots being used as an adjective.  This method wouldn't return perfect results, though.  For example, we could improve our results by checking for instances of \"Scots\" being used as an adjective before the word \"dialects,\" for example.\n",
    "\n",
    "We'll wait to dive into this sort of text analysis until a bit later, though!  \n",
    "\n",
    "#### 0.1 Dataset Size\n",
    "First, let's get a sense of how much data (in this case, text) we have in the *Lewis Grassic Gibbon First Editions* (LGG) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total...\n",
      "  Characters in LGG Data: 7068578\n",
      "  Tokens in LGG Data: 1506095\n",
      "  Sentences in LGG Data: 70821\n",
      "  Files in LGG Data: 16\n"
     ]
    }
   ],
   "source": [
    "def corpusStatistics(plaintext_corpus_read_lists):\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "    total_sents = 0\n",
    "    total_files = 0\n",
    "    for fileid in plaintext_corpus_read_lists.fileids():\n",
    "        total_chars += len(plaintext_corpus_read_lists.raw(fileid))\n",
    "        total_tokens += len(plaintext_corpus_read_lists.words(fileid))\n",
    "        total_sents += len(plaintext_corpus_read_lists.sents(fileid))\n",
    "        total_files += 1\n",
    "    print(\"Estimated total...\")\n",
    "    print(\"  Characters in LGG Data:\", total_chars)\n",
    "    print(\"  Tokens in LGG Data:\", total_tokens)\n",
    "    print(\"  Sentences in LGG Data:\", total_sents)\n",
    "    print(\"  Files in LGG Data:\", total_files)\n",
    "\n",
    "corpusStatistics(wordlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I've print ``Tokens`` rather than words, though the NLTK method used to count those was ``.words()``.  This is because words in NLTK include punctuation marks and digits, in addition to alphabetic words.\n",
    "\n",
    "#### 0.2 Identifying Subsets of the Data\n",
    "Next, we'll create two subsets of the data, one for each journal.  To do so we first need to load the inventory (CSV file) that lists which file name corresponds with which journal.  When you open the CSV file in Microsoft Excel or a text editor, you can see that there are no column names.  The Python library [Pandas](https://pandas.pydata.org/docs/), which reads CSV files, calls these column names the ``header``.  When we use Pandas to read the inventory, we'll create our own header by specifying that the CSV file as ``None`` and providing a list of column ``names``.\n",
    "\n",
    "When Pandas (abbreviated ``pd`` when we loaded libraries in the first cell of this notebook) reads a CSV file, it creates a table called a **dataframe** from that data.  Let's see what the Gibbon inventory dataframe looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>205174241.txt</td>\n",
       "      <td>Niger - R.176.i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>205174242.txt</td>\n",
       "      <td>Thirteenth disciple - Vts.137.d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>205174243.txt</td>\n",
       "      <td>Three go back - Vts.152.f.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205174244.txt</td>\n",
       "      <td>Calends of Cairo - Vts.153.c.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205174245.txt</td>\n",
       "      <td>Lost trumpet - Vts.143.j.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>205174246.txt</td>\n",
       "      <td>Image and superscription - Vts.118.l.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>205174247.txt</td>\n",
       "      <td>Spartacus - Vts.6.k.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>205174248.txt</td>\n",
       "      <td>Persian dawns, Egyptian nights - Vts.148.d.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>205174249.txt</td>\n",
       "      <td>Scots quair - Cloud howe - NF.523.b.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>205174250.txt</td>\n",
       "      <td>Scots quair - Grey granite - NF.523.b.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>205174251.txt</td>\n",
       "      <td>Scots quair - Sunset song - NF.523.b.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>205174252.txt</td>\n",
       "      <td>Hanno, or, The future of exploration - S.114.j.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>205174253.txt</td>\n",
       "      <td>Nine against the unknown - S.72.d.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>205174254.txt</td>\n",
       "      <td>Conquest of the Maya - S.60.c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>205174255.txt</td>\n",
       "      <td>Gay hunter - Vts.215.j.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>205202834.txt</td>\n",
       "      <td>Stained radiance - T.204.f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fileid                                              title\n",
       "0   205174241.txt                                    Niger - R.176.i\n",
       "1   205174242.txt                    Thirteenth disciple - Vts.137.d\n",
       "2   205174243.txt                       Three go back - Vts.152.f.22\n",
       "3   205174244.txt                    Calends of Cairo - Vts.153.c.16\n",
       "4   205174245.txt                         Lost trumpet - Vts.143.j.8\n",
       "5   205174246.txt            Image and superscription - Vts.118.l.16\n",
       "6   205174247.txt                            Spartacus - Vts.6.k.19 \n",
       "7   205174248.txt       Persian dawns, Egyptian nights - Vts.148.d.8\n",
       "8   205174249.txt             Scots quair - Cloud howe - NF.523.b.30\n",
       "9   205174250.txt           Scots quair - Grey granite - NF.523.b.31\n",
       "10  205174251.txt            Scots quair - Sunset song - NF.523.b.29\n",
       "11  205174252.txt  Hanno, or, The future of exploration - S.114.j.21\n",
       "12  205174253.txt               Nine against the unknown - S.72.d.10\n",
       "13  205174254.txt                      Conquest of the Maya - S.60.c\n",
       "14  205174255.txt                          Gay hunter - Vts.215.j.26\n",
       "15  205202834.txt                         Stained radiance - T.204.f"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/nls-text-gibbon/gibbon-inventory.csv', header=None, names=['fileid', 'title'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only have 16 files, we returned the entire dataframe above.  With larger dataframes you may wish to use  ``df.head()`` or ``df.tail()`` to print only the first 5 or last 5 rows of your CSV file (both of which will include the column names in the dataframe's header).\n",
    "\n",
    "Now that we created a dataframe, if we want to determine the title of a Gibbon work based on it's file ID, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a list of all file IDs\n",
    "fileids = list(df[\"fileid\"])\n",
    "print(\"List of file IDs:\\n\", fileids)\n",
    "print()\n",
    "\n",
    "# obtain a list of all titles\n",
    "titles = list(df[\"title\"])\n",
    "print(\"List of titles:\\n\", titles)\n",
    "print()\n",
    "\n",
    "# create a dictionary where the keys are file IDs and the values are titles\n",
    "lgg_dict = dict(zip(fileids, titles))\n",
    "print(\"Dictionary of file IDs and titles:\\n\", lgg_dict)\n",
    "print()\n",
    "\n",
    "# pick a file ID by its index number\n",
    "a_file_id = fileids[10]\n",
    "\n",
    "# get the title corresponding with the file ID in the dataframe\n",
    "print(\"The title for the file ID at index 10:\\n\", lgg_dict[a_file_id])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK stores the lists of tokens in the corpus_tokens variable we created by the file IDs, so it's useful to be able to match the file IDs with their book titles!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning and Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Tokenisation\n",
    "Variables that store the word tokens and sentence tokens in our dataset will be useful for future analysis.  Let's create those now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsSents(plaintext_corpus_read_lists):\n",
    "    all_words = []\n",
    "    all_sents = []\n",
    "    for fileid in plaintext_corpus_read_lists.fileids():\n",
    "        \n",
    "        file_words = plaintext_corpus_read_lists.words(fileid)\n",
    "        all_words += [str(word) for word in file_words if word.isalpha()]\n",
    "        \n",
    "        file_sents = sent_tokenize(plaintext_corpus_read_lists.raw(fileid))  #plaintext_corpus_read_lists.sents(fileid)\n",
    "        all_sents += [str(sent) for sent in file_sents]\n",
    "        \n",
    "    return all_words, all_sents\n",
    "        \n",
    "lgg_words, lgg_sents = getWordsSents(wordlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some types of analysis, such as identifying people and places named in Gibbon's works, maintaining the original capitalization is important.  For other types of analysis, such as analysing the vocabulary of Gibbon's works, standardising words by making them lowercase is important.  Let's create a lowercase list of words in the LGG dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgg_words_lower = [word.lower() for word in lgg_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r', 'o', 'first', 'journey', 'national', 'library', 'of', 'scotland', 'â', 'timbuctoo', 'niger', 'by', 'the', 'same', 'author', 'novels', 'forming', 'the', 'trilogy', 'a']\n",
      "['R', 'o', 'First', 'journey', 'National', 'Library', 'of', 'Scotland', 'â', 'TIMBUCTOO', 'NIGER', 'BY', 'THE', 'SAME', 'AUTHOR', 'Novels', 'forming', 'the', 'trilogy', 'A']\n"
     ]
    }
   ],
   "source": [
    "print(lgg_words_lower[0:20])\n",
    "print(lgg_words[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect!\n",
    "\n",
    "#### 1.2 Reducing to Root Forms\n",
    "In addition to tokenisation, **stemming** is a method of standardising, or \"normalising,\" text.  Stemming reduces words to their root form by removing suffixes.  For example, the word \"troubling\" has a stem of \"troubl.\"  NLTK has two types of stemmers that use different algorithms to determine what the root of a word is.  \n",
    "\n",
    "The stemming algorithms below can take several minutes to run, so two are provided below with one commented out (the lines begin with ``#``) so it won't run.  If you'd like to see how the stemming algorithms differ, uncomment the lines by highlighting them and pressing ``cmd`` + ``/``.\n",
    "\n",
    "First, though, let's see what stems of LGG data look like with the Porter Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem the text (reduce words to their root, whether or not the root is a word itself\n",
    "porter = nltk.PorterStemmer()\n",
    "porter_stemmed = [porter.stem(t) for t in lower_str_tokens if t.isalpha()]  # only include alphabetic tokens\n",
    "print(porter_stemmed[500:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lancaster = nltk.LancasterStemmer()\n",
    "# lancaster_stemmed = [lancaster.stem(t) for t in lower_str_tokens if t.isalpha()] # only include alphabetic tokens\n",
    "# print(lancaster_stemmed[500:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Try It!</b> Uncomment the lines of code in the cell above by removing the `#` before each line to see how a different stemming algorithm works: the Lancaster Stemmer.  What differences do you notice in the sample of stems that are returned?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to reducing words to their root is to **lemmatise** tokens.  NLTK's WordNet Lemmatizer reduces a token to its root *only* if the reduction of the token results in a word that's recognized as an English word in WordNet.  Here's what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the text (reduce words to their root ONLY if the root is considered a word in WordNet)\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatised = [wnl.lemmatize(t) for t in lower_str_tokens if t.isalpha()]  # only include alphabetic tokens\n",
    "print(lemmatised[500:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Parts of Speech\n",
    "To study the linguistic style of text, analysing the **parts of speech** and their patterns in sentences can be useful.  NLTK has a method for tagging tokens with a part of speech in a sentence.  Let's do that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag parts of speech in sentences\n",
    "pos_tagged = [nltk.pos_tag(sent) for sent in lgg_sents]\n",
    "print(pos_tagged[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK uses abbreviations to identify parts of speech, such as:\n",
    "* `NN` = singular noun, `NNS` = plural noun, `NNP` = singular proper noun, `NNPS` = plural proper noun\n",
    "* `IN` = preposition\n",
    "* `TO` = preposition or infinitive marker\n",
    "* `DT` = determiner\n",
    "* `CC` = coordinating conjunction\n",
    "* `JJ` = adjective\n",
    "* `VB` = verb\n",
    "* `RB` = adverb\n",
    "\n",
    "More abbreviations are explained [here](https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/) or can be queried with `nltk.help.upenn_tagset('TAG')` (replace `TAG` with an NLTK part of speech abbreviation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summary Statistics\n",
    "[Code cells in this section will have one function each, preceded with comments in a markdown cell narrating the summarization process]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Frequencies and Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Uniqueness and Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here - most frequent words, sentence structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Analysis \n",
    "(this section will be included for 2-3 datasets)\n",
    "[Code cells in this section will have one function each, preceded with comments in a markdown cell posing an exploratory research question]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 [exploratory research question 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 [exploratory research question 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
