{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Medical History of British India as Data\n",
    "Created in July-September 2020 for the National Library of Scotland's Data Foundry by Lucy Havens, Digital Library Research Intern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the *A Medical History of British India* Dataset\n",
    "The dataset consists of 468 official publications from British India, mainly from 1850-1950, that report on public health, disease mapping, vaccination efforts, veterinary experiments, and other medical topics.  The publications are a subset of a larger collection of 40,000 volumes that report on the administration of British India.  The Wellcome Trust funded the digitisation of the medical history volumes in this dataset.\n",
    "* Data format: digitised text\n",
    "* Data creation process: Optical Character Recognition (OCR) and manual cleaning\n",
    "* Data source: https://data.nls.uk/data/digitised-collections/a-medical-history-of-british-india/\n",
    "***\n",
    "### Table of Contents\n",
    "0. [Preparation](#0.-Preparation)\n",
    "1. [Data Cleaning and Standardisation](#1.-Data-Cleaning-and-Standardisation)\n",
    "2. [Summary Statistics](#2.-Summary-Statistics)\n",
    "3. [Exploratory Analysis](#3.-Exploratory-Analysis)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preparation\n",
    "Import libraries to use for cleaning, summarising and exploring the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# To prevent SSL certificate failure\n",
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "    getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Libraries for data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Libraries for visualization\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Libraries for text analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('tagsets')  # part of speech tags\n",
    "from nltk.draw.dispersion import dispersion_plot as displt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nls-text-indiaPapers folder (downloadable as *Just the text* data from the website at the top of this notebook) contains TXT files of digitised text, with numerical names, as well as a CSV inventory file and a TXT ReadMe file.  Load only the TXT files of digitised text and **tokenise** the text (which splits a string into separate words and punctuation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No', '.', '1111', '(', 'Sanitary', '),', 'dated', 'Ootacamund', ',', 'the']\n"
     ]
    }
   ],
   "source": [
    "corpus_folder = 'data/nls-text-indiaPapers/'\n",
    "wordlists = PlaintextCorpusReader(corpus_folder, '\\d.*', encoding='latin1')\n",
    "corpus_tokens = wordlists.words()\n",
    "print(corpus_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you'd like to see how to specify a single TXT file to load as data, check out the Jupyter Notebook for the Britain and UK Handbooks!*\n",
    "\n",
    "It's hard to get a sense of how accurately the text has been digitised from this list of 10 tokens, so let's look at one of these words in context.  To see phrases in which \"India\" is used, we can use the concordance() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-15232e738d61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcordance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'India'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# by default NLTK's concordance method displays 25 lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/text.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokens, name)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \"\"\"\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_COPY_TOKENS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pieces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;31m# Iterate to the end of the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;31m# Get everything we can from this piece.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_tok\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[1;32m    298\u001b[0m                 \u001b[0;34m\"block reader %s() should return list or tuple.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/corpus/reader/plaintext.py\u001b[0m in \u001b[0;36m_read_word_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Read 20 lines at a time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/regexp.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# If our regexp matches tokens, use re.findall:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = Text(corpus_tokens)\n",
    "t.concordance('India', lines=20)  # by default NLTK's concordance method displays 25 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *A Medical History of British India* (MHBI) dataset has been digitised and then manually corrected for errors in the digitisation process, so we can be pretty confident in the quality of the text for this dataset.\n",
    "\n",
    "Let's find out just how much text and just how many files we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpusStatistics(plaintext_corpus_read_lists):\n",
    "    total_chars = 0\n",
    "    total_tokens = 0\n",
    "    total_sents = 0\n",
    "    total_files = 0\n",
    "    for fileid in plaintext_corpus_read_lists.fileids():\n",
    "        total_chars += len(plaintext_corpus_read_lists.raw(fileid))\n",
    "        total_tokens += len(plaintext_corpus_read_lists.words(fileid))\n",
    "        total_sents += len(plaintext_corpus_read_lists.sents(fileid))\n",
    "        total_files += 1\n",
    "    print(\"Total...\")\n",
    "    print(\"  Characters in MHBI Data:\", total_chars)\n",
    "    print(\"  Tokens in MHBI Data:\", total_words)\n",
    "    print(\"  Sentences in MHBI Data:\", total_sents)\n",
    "    print(\"  Files in MHBI Data:\", total_files)\n",
    "\n",
    "corpusStatistics(wordlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I've print ``Tokens`` rather than words, though the NLTK method used to count those was ``.words()``.  This is because words in NLTK include punctuation marks and digits, in addition to alphabetic words.\n",
    "\n",
    "The `fileids` are the names of the files in the data's source folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileids = list(wordlists.fileids())\n",
    "fileids[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the inventory CSV file from the source folder to match the titles of the papers to the corresponding `fileid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/nls-text-indiaPapers/indiaPapers-inventory.csv', header=None, names=['fileid', 'title'])\n",
    "df.head()  # prints the first 5 rows (df.tail() prints the last 5 rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a list of the titles from the dataframe column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = list(df['title'])\n",
    "titles[0:3]   # Display the first three titles (from index 0 up to but not including index 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables that store the characters, words, and sentences in our dataset will be useful for future analysis.  Let's create those now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCharsWordsSents(plaintext_corpus_read_lists, fileids):\n",
    "    all_chars = []\n",
    "    chars_by_file = dict.fromkeys(fileids)\n",
    "    all_words = []\n",
    "    words_by_file = dict.fromkeys(fileids)\n",
    "    all_words_lower = []\n",
    "    words_lower_by_file = dict.fromkeys(fileids)\n",
    "    all_sents = []\n",
    "    sents_by_file = dict.fromkeys(fileids)\n",
    "    for fileid in plaintext_corpus_read_lists.fileids():\n",
    "        \n",
    "        file_chars = plaintext_corpus_read_lists.raw(fileid)\n",
    "        all_chars += [str(char).lower() for char in file_chars]\n",
    "        chars_by_file[fileid] = all_chars\n",
    "        \n",
    "        file_words = plaintext_corpus_read_lists.words(fileid)\n",
    "        all_words_lower += [str(word).lower() for word in file_words if word.isalpha()]\n",
    "        words_lower_by_file[fileid] = all_words_lower\n",
    "        all_words += [str(word) for word in file_words  if word.isalpha()]\n",
    "        words_by_file[fileid] = all_words\n",
    "        \n",
    "        file_sents = sent_tokenize(plaintext_corpus_read_lists.raw(fileid))  #plaintext_corpus_read_lists.sents(fileid)\n",
    "        all_sents += [str(sent) for sent in file_sents]\n",
    "        sents_by_file[fileid] = all_sents\n",
    "        \n",
    "    return all_chars, chars_by_file, all_words, words_by_file, all_words_lower, words_lower_by_file, all_sents, sents_by_file\n",
    "        \n",
    "mhbi_chars, mhbi_file_chars, mhbi_words, mhbi_file_words, mhbi_words_lower, mhbi_file_lower_words, mhbi_sents, mhbi_file_sents = getCharsWordsSents(wordlists, fileids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the function worked as expected, we can run some quick tests with the output lists and dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mhbi_file_chars[fileids[100]][:10])\n",
    "print(mhbi_file_words[fileids[355]][30:40])\n",
    "print(mhbi_file_sents[fileids[-1]][-20:-10])\n",
    "assert(len(mhbi_file_chars) == len(fileids))  # nothing prints if passes, error prints if doesn't pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mhbi_chars[:100])\n",
    "print(mhbi_words[6100:6120])\n",
    "print(mhbi_sents[-5:])\n",
    "assert(len(mhbi_words_lower) == len(mhbi_words))  # nothing prints if passes, error prints if doesn't pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good!\n",
    "\n",
    "### 1. Data Cleaning and Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the OCR has already been manually cleaned, we'll focus this section on identifying the roots of words and the parts of speech in sentences, rather than looking for mistakes in the OCR.\n",
    "\n",
    "First let's create lists of strings from the NLTK tokens that we can use in future analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tokens = [str(word) for word in corpus_tokens]\n",
    "assert(type(str_tokens[0]) == str)  # quick test to make sure the output is as expected\n",
    "print(str_tokens[0:10])\n",
    "\n",
    "# Lowercase text\n",
    "lower_str_tokens = [t.lower() for t in str_tokens]\n",
    "print(lower_str_tokens[-10:])\n",
    "\n",
    "# Exclude stop words (i.e. the, a, is) - note that the input text must be lowercased!\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "no_stopwords = [t for t in lower_str_tokens if not t in eng_stopwords]\n",
    "print(no_stopwords[500:510])\n",
    "assert(len(no_stopwords) < len(str_tokens))\n",
    "\n",
    "# Alphabetic tokens only (exclude digits and punctuation)\n",
    "alpha_tokens = [t for t in str_tokens if t.isalpha()]\n",
    "alpha_tokens_lower = [t for t in lower_str_tokens if t.isalpha()]\n",
    "print(alpha_tokens[1000:1010])\n",
    "assert(len(alpha_tokens_lower) == len(alpha_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll **stem** the tokens, or reduce the tokens to their root.  NLTK has two types of stemmers that use different algorithms to determine what the root of a word is.  Here's a sample of what they look like.\n",
    "\n",
    "Note that this code can take several minutes to run, so you may wish to choose only one stemmer, which is why one of the stemmers has been commented out. You can uncomment the code so that it runs by removing the ``#`` before each line (highlight all lines and then press ``cmd`` + ``/``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem the text (reduce words to their root, whether or not the root is a word itself\n",
    "porter = nltk.PorterStemmer()\n",
    "porter_stemmed = [porter.stem(t) for t in alpha_tokens_lower]\n",
    "print(porter_stemmed[500:600])\n",
    "\n",
    "# lancaster = nltk.LancasterStemmer()\n",
    "# lancaster_stemmed = [lancaster.stem(t) for t in alpha_tokens_lower]\n",
    "# print(lancaster_stemmed[500:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to reducing words to their root is to **lemmatise** tokens.  NLTK's WordNet Lemmatizer reduces a token to its root *only* if the reduction of the token results in a word that's recognised as an English word in WordNet.  Here's what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatise the text (reduce words to their root ONLY if the root is considered a word in WordNet)\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatized = [wnl.lemmatize(t) for t in alpha_tokens_lower]  # only include alphabetic tokens\n",
    "print(lemmatized[500:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study the linguistic style of text, analysing the **parts of speech** and their patterns in sentences can be useful.  NLTK has a method for tagging tokens with a part of speech in a sentence.  Let's do that too (note that this can take several minutes to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag parts of speech in sentences\n",
    "sentences = wordlists.sents()\n",
    "pos_tagged = [nltk.pos_tag(sent) for sent in sentences]\n",
    "print(pos_tagged[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK uses abbreviations to identify parts of speech, such as:\n",
    "* `CD` = numeral, cardinal\n",
    "* `DT` = determiner\n",
    "* `JJ` = adjective\n",
    "* `NN` = singular noun\n",
    "* `RB` = adverb\n",
    "* `VB` = verb\n",
    "\n",
    "More abbreviations are explained [here](https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/) or can be queried with `nltk.help.upenn_tagset('TAG')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Frequencies and Sizes\n",
    "Now that we've created some different cuts of the MHBI dataset, let's start investigating the frequency of terms as they appear across the dataset.  One way to do so is with a **frequency distribution**, which is a line chart that shows how many times a token appears in the dataset.\n",
    "\n",
    "First let's visualise a frequency distribution for *all alphabetical tokens* in the dataset (excluding punctuation and digits) EXCEPT stop words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO REMOVE BEFORE PUBLISHING: https://digital.nls.uk/indiapapers/institutions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter one-letter words, two-letter words, and  stop words out of the list of alphabetic tokens\n",
    "min_three_letters = [t for t in alpha_tokens_lower if len(t) > 2]\n",
    "to_exclude = list(set(stopwords.words('english'))) + [\"per\", \"two\", \"one\", \"also\"]\n",
    "filtered_tokens = [t for t in min_three_letters if not t in to_exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequency distribution for the filtered list of tokens\n",
    "fdist_ft = FreqDist(filtered_tokens)\n",
    "print(\"Total tokens in filtered list:\", fdist_ft.N())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the frequency distribution for a select number of tokens\n",
    "plt.figure(figsize = (30, 20))                # customise the width and height of the plot\n",
    "plt.rc('font', size=20)                       # customise the font size of the title, axes names, and axes labels\n",
    "fdist_ft.plot(20, title='Frequency Distribution of the 20 Most Common Words in the Medical History of British India Dataset (excluding stop words, 2-letter and 3-letter words)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The medical focus is clear from the top 20 words in the MHBI papers, which includes `cases`, `hospital`, `disease`, `veterinary`, `vaccination`, and `plague`.  Also, the frequency distribution suggests the people writing the papers made an effort to summarise what was going on, since the top three tokens, by far, are `total`, `year`, and `number` (perhaps summarizing `cases` by `district`?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a frequency distribution for *stemmed* words to see if differences arise in the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter one-letter words, two-letter words, and  stop words out of the list of porter-stemmed tokens\n",
    "min_three_letters = [s for s in porter_stemmed if len(s) > 2]\n",
    "to_exclude = list(set(stopwords.words('english'))) + [\"per\", \"two\", \"one\", \"also\"]\n",
    "filtered_pstems = [s for s in min_three_letters if not s in to_exclude]\n",
    "# Calculate the frequency distribution for the filtered list of tokens\n",
    "fdist_ps = FreqDist(filtered_pstems)\n",
    "print(\"Total tokens in filtered list:\", fdist_ps.N())\n",
    "# Visualise the frequency distribution for a select number of tokens\n",
    "plt.figure(figsize = (30, 20))                # customise the width and height of the plot\n",
    "plt.rc('font', size=20)                       # customise the font size of the title, axes names, and axes labels\n",
    "fdist_ps.plot(20, title='Frequency Distribution of the 20 Most Common Porter-Stemmed Tokens in the Medical History of British India Dataset (excluding stop words)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope of the frequency distribution line isn't quite so steep when we look at word stems (using the Porter Stemmer, in this case).  This gives us a better sense of how often a topic appears, such as anything to do with vaccines, rather than a specific form of a word, such as vaccination.  The stem `vaccin` occurs with much higher frequency (about 60,000 times) than in the previous two charts (which record its frequency of occurrences at about 25,000 and 30,000 times).  What other differences can you spot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to visualising the frequency distribution of tokens, we can plot the frequency distribution of *parts of speech*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_tags = FreqDist(tag for (word, tag) in pos_tagged)\n",
    "print(fdist_tags.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30, 20))                # customise the width and height of the plot\n",
    "plt.rc('font', size=20)                       # customise the font size of the title, axes names, and axes labels\n",
    "fdist_tags.plot(cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what parts of speech some of the top acronyms stand for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.help.upenn_tagset('RP'))\n",
    "print(nltk.help.upenn_tagset('JJS'))\n",
    "print(nltk.help.upenn_tagset('RBR'))\n",
    "print(nltk.help.upenn_tagset('FW'))\n",
    "print(nltk.help.upenn_tagset('PDT'))\n",
    "print(nltk.help.upenn_tagset('RBS'))\n",
    "print(nltk.help.upenn_tagset('UH'))\n",
    "print(nltk.help.upenn_tagset('WP$'))\n",
    "print(nltk.help.upenn_tagset('LS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Uniqueness and Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to summarise the MHBI dataset is to look at the uniqueness and variety of word usage.  We can obtain the **vocabulary** of the text by creating a set of unique words (alphabetic tokens) that occur in the dataset, as well as creating a set of unique *lemmatised* words that occur in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate words from the text (obtain the vocabulary of the text)\n",
    "t_vocab = set(alpha_tokens)\n",
    "t_vocab_lower = set(alpha_tokens_lower)\n",
    "lemma_vocab = set(lemmatised)\n",
    "print(\"Unique tokens:\", len(t_vocab))\n",
    "print(\"Unique lowercase tokens:\", len(t_vocab_lower))\n",
    "print(\"Unique lemmatised (lowercase) tokens:\", len(lemma_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a data visualisation that illustrates when specific words are used within the MHBI dataset.  This is called a **Lexical Dispersion Plot**.  We'll pick some medical-related stems (the list of `targets`) to see when they appear (based on the Porter Stemmer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "# TO DO: Order papers chronologically!!!  Or by type?  Then compare frequency of stemmed words?\n",
    "# https://stackoverflow.com/questions/52910655/plotting-two-nltk-freqdists\n",
    "##############################################################################################################################\n",
    "\n",
    "##############################################################################################################################\n",
    "# ALSO: try collocations for exploratory analysis below... http://librarycarpentry.org/lc-tdm/12-collocations/index.html\n",
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Analysis (this section will be included for 2-3 datasets)\n",
    "[Code cells in this section will have one function each, preceded with comments in a markdown cell posing an exploratory research question]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Which publications are about cholera?  Leprosy?  Malaria?  Plague?  \"Laboratory Medicine?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here - disinfecting trains for cholera to help stop spread during religious festivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mhbi_bigrams = bigrams(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 How does the language around the people of India change over time?\n",
    "\n",
    "*Especially after the 1857 rebellion when British East India Company rule in India was taken over by the British Crown*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here - terms to search for: native(s), indigenous; patronizing sentiment...\n",
    "# other colonial indicators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 How does the language around mental hospitals change over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  term to search: lunatic asylums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 How are women portrayed?\n",
    "\n",
    "Consider lock(ed) hospitals and escapes from them, prostitutes permitted in army barracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 What is the rhetoric around vaccinations and, more generally, public health?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Public health, field teams, indigenous population, teaching hospitals, vaccination programmes, \n",
    "# military personnel, anti-vaccine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
