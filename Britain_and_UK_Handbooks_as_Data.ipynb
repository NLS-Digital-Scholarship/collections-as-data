{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Britain and UK Handbooks as Data\n",
    "\n",
    "Created in July and August 2020 for the National Library of Scotland's Data Foundry by Lucy Havens, Digital Library Research Intern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the *Britain and UK Handbooks* Dataset\n",
    "The data consists of digitized text from select Britain and UK Handbooks produced between 1954 and 2005.  A central statistics bureau produced the Handbooks each year to communicate information about the UK that would impress international diplomats.\n",
    "* Data format: digitized text\n",
    "* Data creation process: Optical Character Recognition (OCR)\n",
    "* Data source: https://data.nls.uk/digitised-collections/britain-uk-handbooks/\n",
    "\n",
    "Time Permitting: extract and display covers???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preparation\n",
    "Import libraries to use for cleaning, summarizing and exploring the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# To prevent SSL certificate failure\n",
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "    getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Libraries for data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Libraries for visualization\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Libraries for text analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.draw.dispersion import dispersion_plot as displt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nls-text-handbooks folder (downloadable as *Just the text* data from the website at the top of this notebook) contains TXT files of digitized text, with numerical names, as well as a CSV inventory file and a TXT ReadMe file.  Load only the TXT files of digitized text and **tokenize** the text (which splits a string into separate words and punctuation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BRITAIN', '1979', '3W', '+', 'L', 'Capita', '!', 'Edinburgh', 'Population', '5']\n"
     ]
    }
   ],
   "source": [
    "corpus_folder = 'data/nls-text-handbooks/'\n",
    "wordlists = PlaintextCorpusReader(corpus_folder, '\\d.*', encoding='latin1')\n",
    "corpus_tokens = wordlists.words()\n",
    "print(corpus_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to get a sense of how accurately the text has been digitized from this list of 10 tokens, so let's look at one of these words in context.  To see phrases in which \"Edinburgh\" is used, we can use the concordance() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 20 of 2579 matches:\n",
      "BRITAIN 1979 3W + L Capita ! Edinburgh Population 5 , 196 / GOO ENGLAND A\n",
      "ondon WC1V 6HB 13a Castle Street , Edinburgh EH2 3AR 41 The Hayes , Cardiff CF1\n",
      "ield Liverpool Manchester Bradford Edinburgh Bristol Belfast Coventry Cardiff s\n",
      "Counsellors of State ( the Duke of Edinburgh , the four adult persons next in s\n",
      "ments , accompanied by the Duke of Edinburgh , and undertakes lengthy tours in \n",
      "y government bookshops in London , Edinburgh , Cardiff , Belfast , Manchester ,\n",
      "five Scottish departments based in Edinburgh and known as the Scottish Office .\n",
      " is centred in the Crown Office in Edinburgh . The Parliamentary Draftsmen for \n",
      ". The main seat of the court is in Edinburgh where all appeals are heard . All \n",
      " The Court of Session sits only in Edinburgh , and has jurisdiction to deal wit\n",
      "ersities are : Aberdeen , Dundee , Edinburgh , Glasgow , Heriot - Watt ( Edinbu\n",
      "nburgh , Glasgow , Heriot - Watt ( Edinburgh ), St . Andrews , Stirling , and S\n",
      ". Andrews , Glasgow , Aberdeen and Edinburgh from the fifteenth and sixteenth c\n",
      "the Pentland Hills to the south of Edinburgh . Over 98 per cent of the land in \n",
      " , a major commercial centre , and Edinburgh , Scotland s capital , an administ\n",
      " , bife and Dundee , as well as in Edinburgh , where this and other modern indu\n",
      "c Services Station , East Craigs , Edinburgh , provide scientific and technical\n",
      "similar service between London and Edinburgh in May 1978 and the construction o\n",
      "t England and on the route linking Edinburgh , Newcastle upon Tyne , Birmingham\n",
      "e been introduced from Heathrow to Edinburgh and Belfast . Joint shuttle servic\n"
     ]
    }
   ],
   "source": [
    "t = Text(corpus_tokens)\n",
    "t.concordance('Edinburgh', lines=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm guessing `bife` should be `Fife` as it's closely followed by `Dundee`, but overall not so bad!\n",
    "\n",
    "We can also load individual files from the nls-text-handbooks folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GH', '.', 'fl-', '[', 'IASG0', '>', 'J^RSEI', 'nice', ']', 'ROME']\n",
      "\n",
      "['GH.', 'fl-\\n[IASG0>\\nJ^RSEI\\nnice]\\nROME\\nGIBRALTARi\\n[NICOSIA\\nTRIPOLI\\nCAIRO)\\n[KUWAIT\\nBAHREIN\\nXUTTA\\nHONG KONG\\nBOMBAY!', \".DAKAR\\nLgambia\\nRANGO(\\nKHARTOUM I\\nADEN\\nBANGKOI\\nbfrbI\\nrPREETOVVN i'i'V L\\nHARGEISA\\nCOLOMBOâ€™\\nLABUANi\\n^CCRA\\nMogadishu\\nENTEBI\\n.SINGAI\\n[ROBl\\nDJAKARTA*\\nlivingstoJB\\nMAURITIUS\\nJOHANNESBURG^\\noS3\\nCL\\nsou\\nT H E R N\\nROUTES OPERATED By BRITISH OVERSEAS AIRWAYS-BRITISH EUROPEAN AIRWA YS- TRANS-CANADA AIR LINES -Q ANT AS EMPIRE AIRWAYS I\\nMain Commonwealth Air Routes\\nAugust 1954\\nGLASG<\\nEDMONTON,\\nWINNIPEG\\nVANCOI\\nCALGAR1\\njersey:\\nAIONTREAI\\nToronto'\\nCHICAGO!\", 'SAN FRANC ISC<\\nGIBRALTAR!', '[BERMUDA\\nHONOLULU)\\nGRAND CAYMAN\\nDAKAR!']\n"
     ]
    }
   ],
   "source": [
    "file = open('data/nls-text-handbooks/205336772.txt', 'r')\n",
    "sample_text = file.read()\n",
    "sample_tokens = word_tokenize(sample_text)\n",
    "sample_sents = sent_tokenize(sample_text)\n",
    "print(sample_tokens[:10])\n",
    "print()\n",
    "print(sample_sents[:5])\n",
    "# print(sample_sents[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in this Notebook, we're interested in the entire dataset, so we'll use all its files.  Let's find out just how many files, and just how much text, we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total...\n",
      "  Characters in Handbooks Data: 90573254\n",
      "  Words in Handbooks Data: 16606800\n",
      "  Sentences in Handbooks Data: 584618\n",
      "  Files in Handbooks Data: 50\n"
     ]
    }
   ],
   "source": [
    "def corpusStatistics(plaintext_corpus_read_lists):\n",
    "    total_chars = 0\n",
    "    total_words = 0\n",
    "    total_sents = 0\n",
    "    total_files = 0\n",
    "    for fileid in plaintext_corpus_read_lists.fileids():\n",
    "        total_chars += len(plaintext_corpus_read_lists.raw(fileid))\n",
    "        total_words += len(plaintext_corpus_read_lists.words(fileid))\n",
    "        total_sents += len(plaintext_corpus_read_lists.sents(fileid))\n",
    "        total_files += 1\n",
    "    print(\"Total...\")\n",
    "    print(\"  Characters in Handbooks Data:\", total_chars)\n",
    "    print(\"  Words in Handbooks Data:\", total_words)\n",
    "    print(\"  Sentences in Handbooks Data:\", total_sents)\n",
    "    print(\"  Files in Handbooks Data:\", total_files)\n",
    "\n",
    "corpusStatistics(wordlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the 50 files that make up the Handbooks dataset, there are over 90 million characters (which could be words, numbers, punctuation, abbreviations, etc.), over 16 million words, and nearly 600,000 sentences.  Of course, OCR isn't perfect, so these numbers are estimates, not precise totals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables that store the characters, words, and sentences in our dataset will be useful for future analysis.  Let's create those now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCharsWordsSents(plaintext_corpus_read_lists):\n",
    "    all_chars = []\n",
    "    all_words = []\n",
    "    all_words_lower = []\n",
    "    all_sents = []\n",
    "    for fileid in plaintext_corpus_read_lists.fileids():\n",
    "        \n",
    "        file_chars = plaintext_corpus_read_lists.raw(fileid)\n",
    "        all_chars += [str(char).lower() for char in file_chars]\n",
    "        \n",
    "        file_words = plaintext_corpus_read_lists.words(fileid)\n",
    "        all_words_lower += [str(word).lower() for word in file_words if word.isalpha()]\n",
    "        all_words += [str(word) for word in file_words  if word.isalpha()]\n",
    "        \n",
    "        file_sents = sent_tokenize(plaintext_corpus_read_lists.raw(fileid))  #plaintext_corpus_read_lists.sents(fileid)\n",
    "        all_sents += [str(sent) for sent in file_sents]\n",
    "        \n",
    "    return all_chars, all_words, all_words_lower, all_sents\n",
    "        \n",
    "handbooks_chars, handbooks_words, handbooks_words_lower, handbooks_sents = getCharsWordsSents(wordlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'r', 'i', 't', 'a', 'i', 'n', ' ', '1', '9']\n",
      "['o', 'r', 'e', 's', 'b', 'y', '%', 't', '\\r', '\\n']\n",
      "\n",
      "['BRITAIN', 'L', 'Capita', 'Edinburgh', 'Population', 'GOO', 'ENGLAND', 'Area', 'km', 'miles']\n",
      "['AIRWAYS', 'ADEN', 'ALWAYS', 'BAHAMAS', 'AIRWAYS', 'ASSOCIATES', 'August', 'r', 'MORESBY', 'T']\n",
      "\n",
      "['britain', 'l', 'capita', 'edinburgh', 'population', 'goo', 'england', 'area', 'km', 'miles']\n",
      "['airways', 'aden', 'always', 'bahamas', 'airways', 'associates', 'august', 'r', 'moresby', 't']\n",
      "\n",
      "Capita! 1979\n",
      "Capita!q.miles.kmGOO\n",
      "^xt:. i - <1.. i 'i&rr\n",
      "u.\n",
      "(between pp 390 and 391).t structure390);olomgssalaries\n",
      "Central Office of Information. by Reference Division of thees\n",
      "information available up to September 1 978.\n",
      "Information Services in many countries.ovided by the British\n",
      "Stationery Office throughout the world.n sale by Her Majesty's\n",
      "part played by government in the life of the country. thethe\n",
      "Island.fijtle\n",
      "dergus _B.\n",
      ")allycpnnÃ‚Â«in.P!\n",
      "sasdebellingham.eet(Distances\n",
      "BOMBAY!NG^ George Philip A Son, Ltd.hilip & Son, Ltd., London\n",
      "LABI!BOÃ¢Â€Â™r\n",
      "CHICAGO!EAIATED BY BRITISH OVERSEAS AIRWAYS-BRITISH EUROPEAN AIRWA YS-TRANS-CANADA AIR LINES Ã¢Â€Â¢ Q ANT AS EMPIRE AIRWAYS\n",
      "GIBRALTAR!\n",
      "GRAND CAYMAN.\n",
      "MORESBY%TN A1RWA VS Ã¢Â€Â¢ TASMAN EMPIRE AIRWA YS- BRITISH WEST INDIAN AIRWAYS-ADEN ALWAYS BAHAMAS AIRWAYS'* ASSOCIATES\n"
     ]
    }
   ],
   "source": [
    "print(handbooks_chars[:10])\n",
    "print(handbooks_chars[-10:])\n",
    "print()\n",
    "print(handbooks_words[:10])\n",
    "print(handbooks_words[-10:])\n",
    "print()\n",
    "print(handbooks_words_lower[:10])\n",
    "print(handbooks_words_lower[-10:])\n",
    "print()\n",
    "sample_sentences = handbooks_sents[:10] + handbooks_sents[-10:]\n",
    "for s in sample_sentences:\n",
    "    # remove new lines and tabs at the start and end of sentences\n",
    "    s = s.strip('\\n')\n",
    "    s = s.strip('\\t')\n",
    "    # remove new lines and tabs in the middle of sentences\n",
    "    s = s.replace('\\n','')\n",
    "    s = s.replace('\\t','')\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bife` most likely isn't the only word the OCR incorrectly digitized.  To get a sense of how much of the digitized text we can perform meaningful analysis on, let's figure out how many of NLTK's \"words\" are actually recognizable English words.\n",
    "\n",
    "We'll use [WordNet](https://wordnet.princeton.edu/),* a database of English words, to evaluate which of NLTK's \"words\" are not valid English words.\n",
    "\n",
    "***\n",
    "\n",
    "  *Princeton University \"About WordNet.\" WordNet. Princeton University. 2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First,** let's create a list of strings from the words NLTK has identified for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BRITAIN', '1979', '3W', '+', 'L', 'Capita', '!', 'Edinburgh', 'Population', '5']\n"
     ]
    }
   ],
   "source": [
    "str_tokens = [str(word) for word in corpus_tokens]\n",
    "assert(type(str_tokens[0]) == str)  # quick test to make sure the output is as expected\n",
    "print(str_tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are digits and punctuation that won't be recognized as words in WordNet but still provide valuable text data for studying the Handbooks.  For example, in the output above, it looks as though the OCR processed the word `Capital` as `Capita!`, which NLTK has split into two.  Furthermore, the word '1979' is a date that puts the text in context, which would enable one to order information in the text by date.\n",
    "\n",
    "To get an estimate of how accurately OCR digitized the Handbooks, though, we'll use words in the sense that they are recognizable words in the English language.  Let's write a regular expression that can tell us whether a string is a word or abbreviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "isWord = re.compile('[a-zA-z.]+')  # include single letters and abbreviations\n",
    "\n",
    "# ----------- TESTING REGEX -----------\n",
    "# print(isWord.match(\"bife\").group())\n",
    "# print(isWord.match(\"U.S.A.\").group())\n",
    "# print(isWord.match(\"W\").group())\n",
    "# print(isWord.match(\"1979\") == None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lastly,** let's use that regular expression to write a function to distinguish words recognizable English words from unrecognizable strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonEnglishWords(list_of_strings):\n",
    "    english_only = []\n",
    "    nonenglish = []\n",
    "    for s in list_of_strings:\n",
    "        test = isWord.match(s)            # fails if has characters other than letters or a period\n",
    "        if (test != None):\n",
    "            passed = test.group()   # get the matching string\n",
    "            if wordnet.synsets(passed):  # see if WordNet recognizes the matching string\n",
    "                english_only.append(passed)\n",
    "            else:\n",
    "                nonenglish.append(passed)\n",
    "        else:\n",
    "            nonenglish.append(passed)\n",
    "    return english_only, nonenglish\n",
    "                \n",
    "recognized, unrecognized = removeNonEnglishWords(str_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total alphabetic words recognized in WordNet: 9665798\n",
      "Total alphabetic words NOT reccognized in WordNet: 4009482\n",
      "Percentage of alphabetic words that are unrecognized in WordNet: 41.481127579947355 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Total alphabetic words recognized in WordNet:\", len(recognized))\n",
    "print(\"Total alphabetic words NOT reccognized in WordNet:\", len(unrecognized))\n",
    "print(\"Percentage of alphabetic words that are unrecognized in WordNet:\", (len(unrecognized)/len(recognized))*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of analysis require different subsets of data, so let's create some now.  To begin, we'll create lists of the handbooks tokens where all words are lowercased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase text\n",
    "lower_str_tokens = [t.lower() for t in str_tokens]\n",
    "lower_recognized = [word.lower() for word in recognized]\n",
    "lower_unrecognized = [word.lower() for word in unrecognized]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a list of the lowercase tokens that excludes stop words, such as `as`, `a`, and `the`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude stop words (i.e. the, a, is) - note that the input text must be lowercased!\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "no_stopwords = [t for t in lower_str_tokens if not t in eng_stopwords]\n",
    "assert(len(no_stopwords) < len(str_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll **stem** the tokens, or reduce the tokens to their root.  NLTK has two types of stemmers that use different algorithms to determine what the root of a word is.  Here's a sample of what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['intern', 'comput', 'ltd', 'ot', 'miniatur', 'circuit', 'between', 'pp', 'and', 'tate', 'er', 'lyle', 'ltd', 'for', 'sugar', 'base', 'gum', 'between', 'pp', 'and', 'mear', 'bro', 'hold', 'ltd', 'for', 'tower', 'face', 'p', 'unit', 'kingdom', 'atom', 'energi', 'author', 'for', 'nuclear', 'fuel', 'reprocess', 'face', 'p', 'metropolitan', 'polic', 'public', 'branch', 'for', 'traffic', 'control', 'room', 'and', 'inform', 'room', 'face', 'p', 'davi', 'loewi', 'ltd', 'for', 'support', 'structur', 'between', 'pp', 'and', 'introduct', 'britain', 'is', 'the', 'thirtieth', 'offici', 'handbook', 'in', 'the', 'seri', 'prepar', 'and', 'revis', 'each', 'year', 'by', 'refer', 'divis', 'of', 'the', 'central', 'offic', 'of', 'inform', 'the', 'content', 'are', 'base', 'on', 'inform', 'avail', 'up', 'to', 'septemb', 'the', 'handbook', 'is', 'wide', 'known']\n",
      "\n",
      "['intern', 'comput', 'ltd', 'ot', 'miny', 'circuit', 'between', 'pp', 'and', 'tat', 'er', 'lyl', 'ltd', 'for', 'sug', 'bas', 'gum', 'between', 'pp', 'and', 'mear', 'bro', 'hold', 'ltd', 'for', 'tow', 'fac', 'p', 'unit', 'kingdom', 'atom', 'energy', 'auth', 'for', 'nuclear', 'fuel', 'reprocess', 'fac', 'p', 'metropolit', 'pol', 'publ', 'branch', 'for', 'traff', 'control', 'room', 'and', 'inform', 'room', 'fac', 'p', 'davy', 'loewy', 'ltd', 'for', 'support', 'structure', 'between', 'pp', 'and', 'introduc', 'britain', 'is', 'the', 'thirtie', 'off', 'handbook', 'in', 'the', 'sery', 'prep', 'and', 'rev', 'each', 'year', 'by', 'ref', 'divid', 'of', 'the', 'cent', 'off', 'of', 'inform', 'the', 'cont', 'ar', 'bas', 'on', 'inform', 'avail', 'up', 'to', 'septemb', 'the', 'handbook', 'is', 'wid', 'known']\n"
     ]
    }
   ],
   "source": [
    "# Stem the text (reduce words to their root, whether or not the root is a word itself\n",
    "porter = nltk.PorterStemmer()\n",
    "porter_stemmed = [porter.stem(t) for t in lower_str_tokens if t.isalpha()]  # only include alphabetic tokens\n",
    "print(porter_stemmed[500:600])\n",
    "print()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "lancaster_stemmed = [lancaster.stem(t) for t in lower_str_tokens if t.isalpha()] # only include alphabetic tokens\n",
    "print(lancaster_stemmed[500:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to reducing words to their root is to **lemmatize** tokens.  NLTK's WordNet Lemmatizer reduces a token to its root *only* if the reduction of the token results in a word that's recognized as an English word in WordNet.  Here's what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['international', 'computer', 'ltd', 'ot', 'miniature', 'circuit', 'between', 'pp', 'and', 'tate', 'er', 'lyle', 'ltd', 'for', 'sugar', 'based', 'gum', 'between', 'pp', 'and', 'mears', 'bros', 'holding', 'ltd', 'for', 'tower', 'facing', 'p', 'united', 'kingdom', 'atomic', 'energy', 'authority', 'for', 'nuclear', 'fuel', 'reprocessing', 'facing', 'p', 'metropolitan', 'police', 'publicity', 'branch', 'for', 'traffic', 'control', 'room', 'and', 'information', 'room', 'facing', 'p', 'davy', 'loewy', 'ltd', 'for', 'support', 'structure', 'between', 'pp', 'and', 'introduction', 'britain', 'is', 'the', 'thirtieth', 'official', 'handbook', 'in', 'the', 'series', 'prepared', 'and', 'revised', 'each', 'year', 'by', 'reference', 'division', 'of', 'the', 'central', 'office', 'of', 'information', 'the', 'content', 'are', 'based', 'on', 'information', 'available', 'up', 'to', 'september', 'the', 'handbook', 'is', 'widely', 'known']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the text (reduce words to their root ONLY if the root is considered a word in WordNet)\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemmatized = [wnl.lemmatize(t) for t in lower_str_tokens if t.isalpha()]  # only include alphabetic tokens\n",
    "print(lemmatized[500:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study the linguistic style of text, analyzing the **parts of speech** and their patterns in sentences can be useful.  NLTK has a method for tagging tokens with a part of speech in a sentence.  Let's do that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag parts of speech in sentences\n",
    "sentences = wordlists.sents()  # sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "pos_tagged = [nltk.pos_tag(sent) for sent in sentences]\n",
    "print(pos_tagged[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK uses abbreviations to identify parts of speech, such as:\n",
    "* `NN` = singular noun, `NNS` = plural noun, `NNP` = singular proper noun, `NNPS` = plural proper noun\n",
    "* `IN` = preposition\n",
    "* `TO` = preposition or infinitive marker\n",
    "* `DT` = determiner\n",
    "* `CC` = coordinating conjunction\n",
    "* `JJ` = adjective\n",
    "* `VB` = verb\n",
    "* `RB` = adverb\n",
    "\n",
    "More abbreviations are explained [here](https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/) or can be queried with `nltk.help.upenn_tagset('TAG')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Frequencies and Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created some different cuts of the Handbooks dataset, let's start investigating the frequency of terms as they appear across the dataset.  One way to do so is with a **frequency distribution**, which is a line chart that shows how many times a token appears in the dataset.\n",
    "\n",
    "First let's create visualize a frequency distribution for all tokens in the dataset EXCEPT stop words, punctuation, and digits.  As you can see below, we can define our own stop words to exclude from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_two_letters = [t for t in no_stopwords if len(t) > 2]\n",
    "to_exclude = set(list(string.punctuation) + list(string.digits) + ['also', 'per', '000', 'one', 'many', 'may', 'two', 'see'])\n",
    "filtered_tokens = [t for t in min_two_letters if not t in to_exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_ft = FreqDist(filtered_tokens)\n",
    "fdist_ft.N()  # count the total tokens after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30, 20))                # customize the width and height of the plot\n",
    "plt.rc('font', size=20)                       # customize the font size of the title, axes names, and axes labels\n",
    "fdist_ft.plot(50, title='Frequency Distribution for 50 Most Common Words in the Handbooks Dataset (excluding stop words)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the same type of plot using tokens that were recognized by WordNet as English words, excluding the same stop words, punctuation, and digits as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_min_two_letters = [t for t in lower_recognized if len(t) > 2]\n",
    "to_exclude = set(stopwords.words('english') + list(string.punctuation) + list(string.digits) + ['also', 'per', '000', 'one', 'many', 'may', 'two', 'see'])\n",
    "filtered_rec_tokens = [t for t in rec_min_two_letters if not t in to_exclude]\n",
    "fdist_ft_rec = FreqDist(filtered_rec_tokens)\n",
    "fdist_ft_rec.N()  # count the total tokens after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30, 20))\n",
    "plt.rc('font', size=20)\n",
    "fdist_ft_rec.plot(50, title='Frequency Distribution for 50 Most Common Tokens among Recognized English Words in the Handbooks Dataset (excluding stop words)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots look very similar!  Nearly all the tokens are repeated in each visualization and they appear in a similar order.  It's also reassuring to see that the top 50 tokens are real words or names, which means those outweigh mistakenly spelled words and names that resulted from the imperfect OCR (the digitization process).\n",
    "\n",
    "Let's make one more frequency distribution, visualizing the occurrences of the top 50 *lemmatized* words in the Handbooks dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_two_letters = [t for t in lemmatized if len(t) > 2]\n",
    "to_exclude = set(stopwords.words('english') + list(string.punctuation) + list(string.digits) + ['also', 'per', '000', 'one', 'many', 'may', 'two', 'see'])\n",
    "filtered_lemma = [t for t in rec_min_two_letters if not t in to_exclude]\n",
    "fdist_lemma = FreqDist(filtered_lemma)\n",
    "plt.figure(figsize = (30, 20))\n",
    "plt.rc('font', size=20)\n",
    "fdist_lemma.plot(50, title='Frequency Distribution for 50 Most Common Lemmatized Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Uniqueness and Variety\n",
    "\n",
    "Another way to summarize the Handbooks dataset is to look at the uniqueness and variety of word usage.  We can obtain the **vocabulary** of the text by creating a set of unique tokens that occur in the dataset, as well as creating a set of unique *lemmatized* tokens that occur in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate tokens from the text (obtain the vocabulary of the text)\n",
    "t_vocab = set(str_tokens)\n",
    "t_vocab_lower = set(lower_str_tokens)\n",
    "lemma_vocab = set(lemmatized)\n",
    "print(\"Unique tokens:\", len(t_vocab))\n",
    "print(\"Unique lowercase tokens:\", len(t_vocab_lower))\n",
    "print(\"Unique lemmatized (lowercase) tokens:\", len(lemma_vocab))\n",
    "print()\n",
    "rec_vocab = set(recognized)\n",
    "rec_vocab_lower = set(lower_recognized)\n",
    "unrec_vocab = set(unrecognized)\n",
    "unrec_vocab_lower = set(lower_unrecognized)\n",
    "print(\"Unique recognized words:\", len(rec_vocab))\n",
    "print(\"Unique recognized lowercase words:\", len(rec_vocab_lower))\n",
    "print(\"Unique unrecognized words:\", len(unrec_vocab))\n",
    "print(\"Unique unrecognized lowercase words:\", len(unrec_vocab_lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary of the entire Handbooks dataset contains 70,922 unique words, 36,780 of which are recognized English words in WordNet. The lemmatized vocabulary of the dataset contains 66,172 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(lemma_vocab)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = Text(corpus_tokens)\n",
    "targets = ['Britain', 'British', 'England', 'English', 'Scotland', 'Scottish', 'Ireland', 'Irish', 'Wales', 'Welsh']\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.rc('font', size=20)\n",
    "displt(corpus_text, targets, ignore_case=True, title='Lexical Dispersion Plot of Scot-related Words in the Handbooks Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm.  The words we chose occur so frequently across the entire dataset that it's difficult to pick out many patterns, though we can se that `Irish` and `Welsh` occur less often that `British` and `Scottish`.  Since the Handbooks dataset contains multiple volumes, we could try picking a subset of volumes, or even a single volume, and visualizing the lexical dispersion for that subset of the Handbooks dataset.  That may reveal more interesting patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 How is Britain and the UK portrayed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Handbooks were written for an international audience to impress people with the success and strength of Britain and the UK.  Let's investigate how Britain and the UK are portrayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Text(corpus_tokens)\n",
    "t.concordance('Britain', lines=10)\n",
    "# t.concordance('GB')\n",
    "# t.concordance('United Kingdom') # no matches\n",
    "# t.concordance('UK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(str_tokens)\n",
    "print(\"Frequency (percentage) of Britain and the UK in Handbooks dataset:\")\n",
    "print(\" - Britain:\", (fdist.freq('Britain'))*100, \"%\")\n",
    "print(\" - GB:\", (fdist.freq('GB'))*100, \"%\")\n",
    "print(\" - UK:\", (fdist.freq('UK'))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations go here - frequency of words over time and per volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 How is Scotland portrayed?\n",
    "How does the portrayal of Britain and the UK compare or contrast with the portrayal of Scotland?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.concordance('Scotland', lines=10)\n",
    "# t.concordance('Ireland')  # re.search(.{20}'(?<!Northern )Ireland.{20}')\n",
    "# t.concordance('Wales')\n",
    "# t.concordance('England')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scot_strings = [s for s in lower_str_tokens if (re.search('scot$', s) or re.search('scot[tcls]+', s))]\n",
    "print(\"Total tokens related to Scotland:\", len(scot_strings))\n",
    "unique_scot = set(scot_strings)\n",
    "print(\"Unique tokens related to Scotland:\", len(unique_scot))\n",
    "print(unique_scot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(str_tokens)\n",
    "print(\"Frequency (percentage) of Nations in Handbooks dataset:\")\n",
    "print(\" - Scotland:\", (fdist.freq('Scotland'))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_text = Text(corpus_tokens)\n",
    "targets = list(unique_scot)\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.rc('font', size=20)\n",
    "displt(corpus_text, targets, ignore_case=True, title='Lexical Dispersion Plot of Scot-related Words in the Handbooks Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Identifying Tables to Transform to Dataframes\n",
    "The Handbooks contain many tables of statistics.  Could we identify where those are in the text and recreate the tables as DataFrames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Text(str_tokens)\n",
    "t.concordance('TABLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [s for s in handbooks_sents if re.search('table (\\d){1}(\\d)* :', s)]\n",
    "print(\"Tables in Handbooks:\", len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
