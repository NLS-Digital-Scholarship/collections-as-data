{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ladies' Edinburgh Debating Society as Data\n",
    "Created August-September 2020 for the National Library of Scotland's Data Foundry by Lucy Havens, Digital Library Research Intern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the *Ladies' Edinburgh Debating Society* Dataset\n",
    "The Ladies' Edinburgh Debating Society (LEDS) was founded by women in 1865 who were members of the upper-middle and higher classes at a time when women had limited higher education opportunities.  Members went on to play significant roles in education, suffrage, philanthropy, and anti-slavery efforts.  The LEDS Dataset contains digitized text from all volumes of two journals the Society published: \"The Attempt\" and \"The Ladies' Edinburgh Magazine.\"  The first journal contains 10 volumes published from 1865 through 1874.  The second journal contains six volumes published from 1875 through 1880.  \n",
    "\n",
    "The Ladies' Edinburgh Debating Society, also known as the Edinburgh Essay Society and the Ladies' Edinburgh Essay Society, was dissolved in 1935.  A year later, in 1936, the National Library of Scotland acquired the volumes that were digitized in this dataset.\n",
    "* Data format: digitised text\n",
    "* Data creation process: Optical Character Recognition (OCR)\n",
    "* Data source: https://data.nls.uk/data/digitised-collections/edinburgh-ladies-debating-society/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preparation\n",
    "Import libraries to use for cleaning, summarizing and exploring the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# To prevent SSL certificate failure\n",
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "    getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Libraries for data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Libraries for visualization\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Libraries for text analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('tagsets')  # part of speech tags\n",
    "from nltk.draw.dispersion import dispersion_plot as displt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nls-text-ladiesDebating folder (downloadable as *Just the text* data from the website at the top of this notebook) contains TXT files of digitized text, with numerical names, as well as a CSV inventory file and a TXT ReadMe file.  Load only the TXT files of digitized text and **tokenize** the text (which splits a string into separate words and punctuation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â', '\\x80¢*', 'â', '\\x80¢', 'UL', '.', 'u', '^\\\\,', 'THE', 'ATTEMPT']\n"
     ]
    }
   ],
   "source": [
    "corpus_folder = 'data/nls-text-ladiesDebating/'\n",
    "wordlists = PlaintextCorpusReader(corpus_folder, '\\d.*', encoding='latin1')\n",
    "corpus_tokens = wordlists.words()\n",
    "print(corpus_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you'd like to see how to specify a single TXT file to load as data, check out the Jupyter Notebook for the Britain and UK Handbooks!*\n",
    "\n",
    "It's hard to get a sense of how accurately the text has been digitized from this list of 10 tokens, so let's look at one of these words in context.  To see phrases in which \"Edinburgh\" is used, we can use the concordance() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 2079 matches:\n",
      "UM MELIORIS MVl .\" FEINTED FOR THE EDINBURGH ESSAY SOCIETY . EEID & SON , SHORE\n",
      "atriculated into the University of Edinburgh , where he graduated in Arts , aft\n",
      "pany has been making a stir in the Edinburgh world ), he was making a long spee\n",
      " May proÂ ¬ verbially favours us , Edinburgh holds May one of the dearest of th\n",
      "on of Ecclesiastical Courts met in Edinburgh this month are no less noteworthy \n",
      "wning glory of the month of May in Edinburgh , supplying plenty of gaiety and g\n",
      "go and find out .\" Tlie hair of an Edinburgh cab - owner would stand on end did\n",
      "onal Gallery . Possibly some of my Edinburgh friends may not have had the oppor\n",
      "NE CONDUCTED BY THE MEMBERS OF THE EDINBURGH ESSAY SOCIETY . VOLUME III . \" AUS\n",
      "M MELIORIS uEVI .\" PRINTED FOR THE EDINBURGH ESSAY SOCIETY . COLSTON & SON , ED\n"
     ]
    }
   ],
   "source": [
    "t = Text(corpus_tokens)\n",
    "t.concordance('Edinburgh', lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has not been manually cleaned after OCR digitized text from \"The Attempt\" and \"The Ladies' Edinburgh Magazine\" so it's not surprising to see some non-words appear in the concordance.\n",
    "\n",
    "Before we do much analysis, let's get a sense of how much data we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total...\n",
      "  Characters in LEDS Data: 15096132\n",
      "  Words in LEDS Data: 3145535\n",
      "  Sentences in LEDS Data: 108011\n",
      "  Files in LEDS Data: 16\n"
     ]
    }
   ],
   "source": [
    "def corpusStatistics(plaintext_corpus_read_lists):\n",
    "    total_chars = 0\n",
    "    total_words = 0\n",
    "    total_sents = 0\n",
    "    total_files = 0\n",
    "    \n",
    "    # fileids are the TXT file names in the nls-text-ladies-Debating folder:\n",
    "    for fileid in plaintext_corpus_read_lists.fileids():\n",
    "        total_chars += len(plaintext_corpus_read_lists.raw(fileid))\n",
    "        total_words += len(plaintext_corpus_read_lists.words(fileid))\n",
    "        total_sents += len(plaintext_corpus_read_lists.sents(fileid))\n",
    "        total_files += 1\n",
    "    \n",
    "    print(\"Total...\")\n",
    "    print(\"  Characters in LEDS Data:\", total_chars)\n",
    "    print(\"  Words in LEDS Data:\", total_words)\n",
    "    print(\"  Sentences in LEDS Data:\", total_sents)\n",
    "    print(\"  Files in LEDS Data:\", total_files)\n",
    "\n",
    "corpusStatistics(wordlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create two subsets of the data, one for each journal.  To do so we first need to load the inventory (CSV file) that lists which file name corresponds with which journal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109857781.txt</td>\n",
       "      <td>Attempt - Volume 1 and Select writings - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103655648.txt</td>\n",
       "      <td>Attempt - Volume 2 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103655649.txt</td>\n",
       "      <td>Attempt - Volume 3 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103655650.txt</td>\n",
       "      <td>Attempt - Volume 4 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103655651.txt</td>\n",
       "      <td>Attempt - Volume 5 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>103655652.txt</td>\n",
       "      <td>Attempt - Volume 6 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>103655653.txt</td>\n",
       "      <td>Attempt - Volume 7 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>103655654.txt</td>\n",
       "      <td>Attempt - Volume 8 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>103655655.txt</td>\n",
       "      <td>Attempt - Volume 9 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>103655656.txt</td>\n",
       "      <td>Attempt - Volume 10 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>103655658.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 1 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>103655659.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 2 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>103655660.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 3 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>103655661.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 4 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>103655662.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 5 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>103655663.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 6 - U.393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fileid                                           title\n",
       "0   109857781.txt  Attempt - Volume 1 and Select writings - U.431\n",
       "1   103655648.txt                      Attempt - Volume 2 - U.431\n",
       "2   103655649.txt                      Attempt - Volume 3 - U.431\n",
       "3   103655650.txt                      Attempt - Volume 4 - U.431\n",
       "4   103655651.txt                      Attempt - Volume 5 - U.431\n",
       "5   103655652.txt                      Attempt - Volume 6 - U.431\n",
       "6   103655653.txt                      Attempt - Volume 7 - U.431\n",
       "7   103655654.txt                      Attempt - Volume 8 - U.431\n",
       "8   103655655.txt                      Attempt - Volume 9 - U.431\n",
       "9   103655656.txt                     Attempt - Volume 10 - U.431\n",
       "10  103655658.txt   Ladies' Edinburgh Magazine - Volume 1 - U.393\n",
       "11  103655659.txt   Ladies' Edinburgh Magazine - Volume 2 - U.393\n",
       "12  103655660.txt   Ladies' Edinburgh Magazine - Volume 3 - U.393\n",
       "13  103655661.txt   Ladies' Edinburgh Magazine - Volume 4 - U.393\n",
       "14  103655662.txt   Ladies' Edinburgh Magazine - Volume 5 - U.393\n",
       "15  103655663.txt   Ladies' Edinburgh Magazine - Volume 6 - U.393"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/nls-text-ladiesDebating/ladiesDebating-inventory.csv', header=None, names=['fileid', 'title'])\n",
    "# Since we only have 16 files, we'll print the entire dataframe.  With larger dataframes\n",
    "# you may wish to use  df.head() or df.tail() to print only the first 5 or last 5 rows\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a two dictionaries of fileids and their associated journal titles, one for The Attempt and one for The Ladies' Edinburgh Magazine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'109857781.txt': 'Attempt - Volume 1 and Select writings - U.431', '103655648.txt': 'Attempt - Volume 2 - U.431', '103655649.txt': 'Attempt - Volume 3 - U.431', '103655650.txt': 'Attempt - Volume 4 - U.431', '103655651.txt': 'Attempt - Volume 5 - U.431', '103655652.txt': 'Attempt - Volume 6 - U.431', '103655653.txt': 'Attempt - Volume 7 - U.431', '103655654.txt': 'Attempt - Volume 8 - U.431', '103655655.txt': 'Attempt - Volume 9 - U.431', '103655656.txt': 'Attempt - Volume 10 - U.431'}\n",
      "{'103655658.txt': \"Ladies' Edinburgh Magazine - Volume 1 - U.393\", '103655659.txt': \"Ladies' Edinburgh Magazine - Volume 2 - U.393\", '103655660.txt': \"Ladies' Edinburgh Magazine - Volume 3 - U.393\", '103655661.txt': \"Ladies' Edinburgh Magazine - Volume 4 - U.393\", '103655662.txt': \"Ladies' Edinburgh Magazine - Volume 5 - U.393\", '103655663.txt': \"Ladies' Edinburgh Magazine - Volume 6 - U.393\"}\n"
     ]
    }
   ],
   "source": [
    "attempts = {}\n",
    "mags = {}\n",
    "for index, row in df.iterrows():\n",
    "    fileid = row['fileid']\n",
    "    title = row['title']\n",
    "    if 'Attempt' in title:\n",
    "        attempts[fileid] = title\n",
    "    else: # if 'Magazine' in title:\n",
    "        mags[fileid] = title\n",
    "\n",
    "print(attempts)\n",
    "print(mags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenient reference of only fileids, we can also create lists from the dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['103655658.txt', '103655659.txt', '103655660.txt', '103655661.txt', '103655662.txt', '103655663.txt']\n"
     ]
    }
   ],
   "source": [
    "attempt_ids = list(attempts.keys())\n",
    "mag_ids = list(mags.keys())\n",
    "print(mag_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the results of the concordance method, OCR doesn't result in perfectly digitized text.  To get a sense of how many mistakes may have been made in the digitization process, we can measure how many words in the LEDS data are recognizable English words according to a list of words used considered valid in the board game [Scrabble](https://raw.githubusercontent.com/jesstess/Scrabble/master/scrabble/sowpods.txt) (as demonstrated in [this example](https://stackoverflow.com/questions/61362891/python-is-there-an-nltk-corpus-for-english-gb-words)).\n",
    "\n",
    "**Step 1:** First we'll create a list of words that are in *strings*, a data format in Python for text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â', '\\x80¢*', 'â', '\\x80¢', 'UL', '.', 'u', '^\\\\,', 'THE', 'ATTEMPT']\n"
     ]
    }
   ],
   "source": [
    "str_tokens = [str(word) for word in corpus_tokens]\n",
    "print(str_tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll create a list of all the alphabetic tokens (removing any tokens that aren't potential English words or mistakenly digitised words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'her', 'friends', 'had', 'no', 'cause', 'to', 'complain', 'of', 'her']\n"
     ]
    }
   ],
   "source": [
    "alpha_tokens = [t for t in str_tokens if t.isalpha()]\n",
    "print(alpha_tokens[1000:1010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some types of analysis, such as named entity recognition, it's helpful to keep original capitalization in tokens.  For other types of analysis, such as frequency distributions and topic modelling, it's helpful to lowercase all tokens, standardizing them.  Let's create lowercased versions of the previous lists of tokens for future analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tokens_lower = [(str(word)).lower() for word in corpus_tokens]\n",
    "alpha_tokens_lower = [t.lower() for t in str_tokens_lower if t.isalpha()]\n",
    "\n",
    "# Check that the capitalized and lowercased lists of tokens are the same length, as expected\n",
    "assert(len(str_tokens_lower) == len(str_tokens))       # an error will be thrown if something went wrong   \n",
    "assert(len(alpha_tokens_lower) == len(alpha_tokens))   # an error will be thrown if something went wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Next, we'll load the Scrabble file of words (which helpfully includes British English spellings!) and create a list of those words as a frozen set, which prevents them from being modified accidentally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in Scrabble list: 267752\n",
      "Sample of English words from the Scrabble list: ['abattoirs', 'abattu', 'abature', 'abatures', 'abaxial', 'abaxile', 'abaya', 'abayas', 'abb', 'abba', 'abbacies', 'abbacy', 'abbas', 'abbatial', 'abbe', 'abbed', 'abbes', 'abbess', 'abbesses', 'abbey']\n"
     ]
    }
   ],
   "source": [
    "file = open('data/scrabble_words.txt', 'r')\n",
    "scrabble_words = file.read().split('\\n')\n",
    "scrabble_words_lower = [word.lower() for word in scrabble_file]\n",
    "\n",
    "assert(len(scrabble_words) == len(scrabble_words_lower))  # the number of words shouldn't change when the list is lowercased\n",
    "\n",
    "print(\"Total words in Scrabble list:\", len(scrabble_words))\n",
    "print(\"Sample of English words from the Scrabble list:\", scrabble_words_lower[100:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Now, returning to the alphabetic tokens from the LEDS corpus, we'll **stem** the tokens to obtain the root forms of the tokens.  Stemming obtains root form of a word.  There are different algorithms that one can use to determine the root of a word; we will use the Porter Stemmer.\n",
    "\n",
    "We'll also stem the Scrabble list.  This stemming process should give us a smaller number of words to compare and should enable tokens in LEDS to be recognized as English words even if they appeared in a different form in the Scrabble list.  Stemming is a way of *standardizing* words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â', 'â', 'ul', 'u', 'the', 'attempt', 'a', 'literari', 'magazin', 'conduct']\n",
      "['abandonedli', 'abandone', 'abandone', 'abandon', 'abandon', 'abandon', 'abandon', 'abandon', 'abandon', 'abandonwar']\n"
     ]
    }
   ],
   "source": [
    "# porter = nltk.PorterStemmer()\n",
    "# leds_porter_stemmed = [porter.stem(t) for t in alpha_tokens_lower]\n",
    "# scrabble_porter_stemmed = [porter.stem(t) for t in scrabble_words_lower]\n",
    "print(leds_porter_stemmed[:10])\n",
    "print(scrabble_porter_stemmed[50:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:**  Lastly, we'll compare the root forms of LEDS tokens to the list of Scrabble words to gauge how many tokens are recognizable English words, AND we'll compare all alphabetic tokens in the LEDS corpus with all the Scrabble words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-d0f1d11a5931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrecognized_stems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mleds_porter_stemmed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mstem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscrabble_porter_stemmed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mrecognized_stems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recognized Stems:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrecognized_stems\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleds_porter_stemmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "recognized_stems = 0\n",
    "for stem in leds_porter_stemmed:\n",
    "    if stem in scrabble_porter_stemmed:\n",
    "        recognized_stems += 1\n",
    "print(\"Recognized Stems:\", (recognized_stems/len(leds_porter_stemmed))*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognized_words = 0\n",
    "for t in alpha_tokens_lower:\n",
    "    if t in scrabble_words_lower:\n",
    "        recognized_words += 1\n",
    "print(\"Recognized Words:\", (recognized_words/len(alpha_tokens_lower))*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summary Statistics\n",
    "[Code cells in this section will have one function each, preceded with comments in a markdown cell narrating the summarization process]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Frequencies and Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Uniqueness and Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here - most frequent words, sentence structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Analysis (this section will be included for 2-3 datasets)\n",
    "[Code cells in this section will have one function each, preceded with comments in a markdown cell posing an exploratory research question]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 [exploratory research question 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 [exploratory research question 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
