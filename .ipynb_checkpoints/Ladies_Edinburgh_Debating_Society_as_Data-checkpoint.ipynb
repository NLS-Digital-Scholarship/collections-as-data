{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ladies' Edinburgh Debating Society as Data\n",
    "Created August-September 2020 for the National Library of Scotland's Data Foundry by Lucy Havens, Digital Library Research Intern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the *Ladies' Edinburgh Debating Society* Dataset\n",
    "The Ladies' Edinburgh Debating Society (LEDS) was founded by women in 1865 who were members of the upper-middle and higher classes at a time when women had limited higher education opportunities.  Members went on to play significant roles in education, suffrage, philanthropy, and anti-slavery efforts.  The LEDS Dataset contains digitized text from all volumes of two journals the Society published: \"The Attempt\" and \"The Ladies' Edinburgh Magazine.\"  The first journal contains 10 volumes published from 1865 through 1874.  The second journal contains six volumes published from 1875 through 1880.  \n",
    "\n",
    "The Ladies' Edinburgh Debating Society, also known as the Edinburgh Essay Society and the Ladies' Edinburgh Essay Society, was dissolved in 1935.  A year later, in 1936, the National Library of Scotland acquired the volumes that were digitized in this dataset.\n",
    "* Data format: digitised text\n",
    "* Data creation process: Optical Character Recognition (OCR)\n",
    "* Data source: https://data.nls.uk/data/digitised-collections/edinburgh-ladies-debating-society/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preparation\n",
    "Import libraries to use for cleaning, summarizing and exploring the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# To prevent SSL certificate failure\n",
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "    getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Libraries for data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Libraries for visualization\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Libraries for text analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('tagsets')  # part of speech tags\n",
    "from nltk.draw.dispersion import dispersion_plot as displt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nls-text-ladiesDebating folder (downloadable as *Just the text* data from the website at the top of this notebook) contains TXT files of digitized text, with numerical names, as well as a CSV inventory file and a TXT ReadMe file.  Load only the TXT files of digitized text and **tokenize** the text (which splits a string into separate words and punctuation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â', '\\x80¢*', 'â', '\\x80¢', 'UL', '.', 'u', '^\\\\,', 'THE', 'ATTEMPT']\n"
     ]
    }
   ],
   "source": [
    "corpus_folder = 'data/nls-text-ladiesDebating/'\n",
    "wordlists = PlaintextCorpusReader(corpus_folder, '\\d.*', encoding='latin1')\n",
    "corpus_tokens = wordlists.words()\n",
    "print(corpus_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you'd like to see how to specify a single TXT file to load as data, check out the Jupyter Notebook for the Britain and UK Handbooks!*\n",
    "\n",
    "It's hard to get a sense of how accurately the text has been digitized from this list of 10 tokens, so let's look at one of these words in context.  To see phrases in which \"Edinburgh\" is used, we can use the concordance() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 10 of 2079 matches:\n",
      "UM MELIORIS MVl .\" FEINTED FOR THE EDINBURGH ESSAY SOCIETY . EEID & SON , SHORE\n",
      "atriculated into the University of Edinburgh , where he graduated in Arts , aft\n",
      "pany has been making a stir in the Edinburgh world ), he was making a long spee\n",
      " May proÂ ¬ verbially favours us , Edinburgh holds May one of the dearest of th\n",
      "on of Ecclesiastical Courts met in Edinburgh this month are no less noteworthy \n",
      "wning glory of the month of May in Edinburgh , supplying plenty of gaiety and g\n",
      "go and find out .\" Tlie hair of an Edinburgh cab - owner would stand on end did\n",
      "onal Gallery . Possibly some of my Edinburgh friends may not have had the oppor\n",
      "NE CONDUCTED BY THE MEMBERS OF THE EDINBURGH ESSAY SOCIETY . VOLUME III . \" AUS\n",
      "M MELIORIS uEVI .\" PRINTED FOR THE EDINBURGH ESSAY SOCIETY . COLSTON & SON , ED\n"
     ]
    }
   ],
   "source": [
    "t = Text(corpus_tokens)\n",
    "t.concordance('Edinburgh', lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has not been manually cleaned after OCR digitized text from \"The Attempt\" and \"The Ladies' Edinburgh Magazine\" so it's not surprising to see some non-words appear in the concordance.\n",
    "\n",
    "#### 0.1 Dataset Size\n",
    "Before we do much analysis, let's get a sense of how much data we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total...\n",
      "  Characters in LEDS Data: 15096132\n",
      "  Words in LEDS Data: 3145535\n",
      "  Sentences in LEDS Data: 108011\n",
      "  Files in LEDS Data: 16\n"
     ]
    }
   ],
   "source": [
    "def corpusStatistics(plaintext_corpus_read_lists):\n",
    "    total_chars = 0\n",
    "    total_words = 0\n",
    "    total_sents = 0\n",
    "    total_files = 0\n",
    "    \n",
    "    # fileids are the TXT file names in the nls-text-ladies-Debating folder:\n",
    "    for fileid in plaintext_corpus_read_lists.fileids():\n",
    "        total_chars += len(plaintext_corpus_read_lists.raw(fileid))\n",
    "        total_words += len(plaintext_corpus_read_lists.words(fileid))\n",
    "        total_sents += len(plaintext_corpus_read_lists.sents(fileid))\n",
    "        total_files += 1\n",
    "    \n",
    "    print(\"Total...\")\n",
    "    print(\"  Characters in LEDS Data:\", total_chars)\n",
    "    print(\"  Words in LEDS Data:\", total_words)\n",
    "    print(\"  Sentences in LEDS Data:\", total_sents)\n",
    "    print(\"  Files in LEDS Data:\", total_files)\n",
    "\n",
    "corpusStatistics(wordlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 Identifying Subsets of the Data\n",
    "Next, we'll create two subsets of the data, one for each journal.  To do so we first need to load the inventory (CSV file) that lists which file name corresponds with which journal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109857781.txt</td>\n",
       "      <td>Attempt - Volume 1 and Select writings - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103655648.txt</td>\n",
       "      <td>Attempt - Volume 2 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103655649.txt</td>\n",
       "      <td>Attempt - Volume 3 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103655650.txt</td>\n",
       "      <td>Attempt - Volume 4 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103655651.txt</td>\n",
       "      <td>Attempt - Volume 5 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>103655652.txt</td>\n",
       "      <td>Attempt - Volume 6 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>103655653.txt</td>\n",
       "      <td>Attempt - Volume 7 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>103655654.txt</td>\n",
       "      <td>Attempt - Volume 8 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>103655655.txt</td>\n",
       "      <td>Attempt - Volume 9 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>103655656.txt</td>\n",
       "      <td>Attempt - Volume 10 - U.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>103655658.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 1 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>103655659.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 2 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>103655660.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 3 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>103655661.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 4 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>103655662.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 5 - U.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>103655663.txt</td>\n",
       "      <td>Ladies' Edinburgh Magazine - Volume 6 - U.393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fileid                                           title\n",
       "0   109857781.txt  Attempt - Volume 1 and Select writings - U.431\n",
       "1   103655648.txt                      Attempt - Volume 2 - U.431\n",
       "2   103655649.txt                      Attempt - Volume 3 - U.431\n",
       "3   103655650.txt                      Attempt - Volume 4 - U.431\n",
       "4   103655651.txt                      Attempt - Volume 5 - U.431\n",
       "5   103655652.txt                      Attempt - Volume 6 - U.431\n",
       "6   103655653.txt                      Attempt - Volume 7 - U.431\n",
       "7   103655654.txt                      Attempt - Volume 8 - U.431\n",
       "8   103655655.txt                      Attempt - Volume 9 - U.431\n",
       "9   103655656.txt                     Attempt - Volume 10 - U.431\n",
       "10  103655658.txt   Ladies' Edinburgh Magazine - Volume 1 - U.393\n",
       "11  103655659.txt   Ladies' Edinburgh Magazine - Volume 2 - U.393\n",
       "12  103655660.txt   Ladies' Edinburgh Magazine - Volume 3 - U.393\n",
       "13  103655661.txt   Ladies' Edinburgh Magazine - Volume 4 - U.393\n",
       "14  103655662.txt   Ladies' Edinburgh Magazine - Volume 5 - U.393\n",
       "15  103655663.txt   Ladies' Edinburgh Magazine - Volume 6 - U.393"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/nls-text-ladiesDebating/ladiesDebating-inventory.csv', header=None, names=['fileid', 'title'])\n",
    "# Since we only have 16 files, we'll print the entire dataframe.  With larger dataframes\n",
    "# you may wish to use  df.head() or df.tail() to print only the first 5 or last 5 rows\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a two dictionaries of fileids and their associated journal titles, one for The Attempt and one for The Ladies' Edinburgh Magazine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'109857781.txt': 'Attempt - Volume 1 and Select writings - U.431', '103655648.txt': 'Attempt - Volume 2 - U.431', '103655649.txt': 'Attempt - Volume 3 - U.431', '103655650.txt': 'Attempt - Volume 4 - U.431', '103655651.txt': 'Attempt - Volume 5 - U.431', '103655652.txt': 'Attempt - Volume 6 - U.431', '103655653.txt': 'Attempt - Volume 7 - U.431', '103655654.txt': 'Attempt - Volume 8 - U.431', '103655655.txt': 'Attempt - Volume 9 - U.431', '103655656.txt': 'Attempt - Volume 10 - U.431'}\n",
      "{'103655658.txt': \"Ladies' Edinburgh Magazine - Volume 1 - U.393\", '103655659.txt': \"Ladies' Edinburgh Magazine - Volume 2 - U.393\", '103655660.txt': \"Ladies' Edinburgh Magazine - Volume 3 - U.393\", '103655661.txt': \"Ladies' Edinburgh Magazine - Volume 4 - U.393\", '103655662.txt': \"Ladies' Edinburgh Magazine - Volume 5 - U.393\", '103655663.txt': \"Ladies' Edinburgh Magazine - Volume 6 - U.393\"}\n"
     ]
    }
   ],
   "source": [
    "attempts = {}\n",
    "mags = {}\n",
    "for index, row in df.iterrows():\n",
    "    fileid = row['fileid']\n",
    "    title = row['title']\n",
    "    if 'Attempt' in title:\n",
    "        attempts[fileid] = title\n",
    "    else: # if 'Magazine' in title:\n",
    "        mags[fileid] = title\n",
    "\n",
    "print(attempts)\n",
    "print(mags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenient reference of only fileids, we can also create lists from the dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['103655658.txt', '103655659.txt', '103655660.txt', '103655661.txt', '103655662.txt', '103655663.txt']\n"
     ]
    }
   ],
   "source": [
    "attempt_ids = list(attempts.keys())\n",
    "mag_ids = list(mags.keys())\n",
    "print(mag_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to standardize, or \"normalize,\" text, with each way providing suitable text for different types of analysis.  For example, to study the vocabulary of a text-based dataset, it's useful to remove punctuation and digits, lowercase the remaining alphabetic words, and then reduce those words to their root form (with stemming or lemmatization - more on this later).  Alternatively, to identify people and places using named entity recognition, it's important to keep capitalization in words and keep words in the context of their sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Tokenization\n",
    "In section **0. Preparation**, we tokenized the LEDS dataset when we created the ``corpus_tokens`` list.  ``corpus_tokens`` contains a list of all words, punctuation, and numbers that appear in the LEDS dataset separated into individual items and organized in the order they appear in the LEDS text files.  In addition to tokenizing words, NLTK also provides methods to tokenize sentences.  This is how we counted the number of sentences in section **0.1 Dataset Size**.\n",
    "\n",
    "Tokenized words are helpful when analyzing the vocabulary of text. Tokenized sentences are helpful when analyzing linguistic patterns of a text.  Let's create lists of tokens as *strings* (String is Python's data format for text) from the LEDS dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â', '\\x80¢*', 'â', '\\x80¢', 'UL', '.', 'u', '^\\\\,', 'THE', 'ATTEMPT']\n"
     ]
    }
   ],
   "source": [
    "str_tokens = [str(word) for word in corpus_tokens]\n",
    "print(str_tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a list of tokens that are most likely to be valid English words by removing non-alphabetic tokens from ``str_tokens`` (e.g. punctuation, numbers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'her', 'friends', 'had', 'no', 'cause', 'to', 'complain', 'of', 'her']\n"
     ]
    }
   ],
   "source": [
    "alpha_tokens = [t for t in str_tokens if t.isalpha()]\n",
    "print(alpha_tokens[1000:1010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create lowercased versions of the previous lists of tokens, which can be useful for studying the vocabulary of a dataset, as explained at the beginning of this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tokens_lower = [(str(word)).lower() for word in corpus_tokens]\n",
    "alpha_tokens_lower = [t.lower() for t in str_tokens_lower if t.isalpha()]\n",
    "\n",
    "# Check that the capitalized and lowercased lists of tokens are the same length, as expected\n",
    "assert(len(str_tokens_lower) == len(str_tokens))       # an error will be thrown if something went wrong   \n",
    "assert(len(alpha_tokens_lower) == len(alpha_tokens))   # an error will be thrown if something went wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated at the start of this section, we can also tokenize sentences.  Tokenizing sentences separates sentences in a text into individual items, important when analyzing sentence structure.  Let's create one list of all sentences in the LEDS corpus, and a dictionary of lists for each file in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: ['He reckoned his jokes as a sportsman would count his head of\\r\\ngame, but the effect was drearily oppressive.', 'Fun must be spontaneous to be delightÂ¬\\r\\nful, and no one enjoys a joke when he feels that the joker is scoring it as a hit or a\\r\\nmiss with painful care.', 'Many of the best things ever said are unrecorded, save in the memories of the\\r\\nhearers ; no one chronicles them, and they go floating about the undercurrents of\\r\\ntlie world of talk, making little speaking circles whenever they come to the surface,\\r\\ntill by constant wear the edge is taken off, and they sink for ever among the fossil\\r\\nB\\r\\n10 THE ATTEMPT\\r\\nwitticisms of bygone ages.', 'One such story I have heard of Tliackeray.', 'One Derby\\r\\nDay he was returning by one of the last trains to London, and saw a little man\\r\\nrushing wildly about the platform, exclaiming as he looked into each full carriage,\\r\\n\" Good gracious me !']\n"
     ]
    }
   ],
   "source": [
    "all_sents = []\n",
    "sents_by_file = dict.fromkeys(wordlists.fileids())\n",
    "# Iterate through each file in the LEDS corpus\n",
    "for fileid in wordlists.fileids():\n",
    "    file_sents = sent_tokenize(wordlists.raw(fileid))\n",
    "    all_sents += [str(sent) for sent in file_sents]\n",
    "    sents_by_file[fileid] = all_sents\n",
    "\n",
    "print(\"Sample:\", all_sents[200:205])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder if the language changed from The Attempt publication and the Edinburgh Ladies Magazine publication?  Let's create lists of all sentences for each of these publications so the language of the two publications can be compared and contrasted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences from The Attempt: 53053\n",
      "Sample: ['Is there no one\\r\\nwhom we could comfort out of our abundance, and whose precious blessings we might\\r\\nhope to deserve!', 'Gretchen,â\\x80\\x9d he exclaimed, rising up as the thought seized him,\\r\\nâ\\x80\\x9c our old neighbour Dorothea can have but little since her loving son was taken\\r\\naway: it is the duty of the rich to help the poor, and rich we are compared to the\\r\\ndesolate widow.', 'What think you, my wife; would it not be seemly that she should\\r\\npartake of our Christmas bounty!â\\x80\\x9d The kindly housewife did not reply in words,\\r\\nbut looking up in her husbandâ\\x80\\x99s face, gave a pleasant smile and approving nod, and\\r\\nwas soon again busily engaged with her knitting.', 'With the somewhat impetuous\\r\\nCarl Holz, to determine anything was at once to do it; so, rising up, he began to\\r\\nprepare for a walk of some four or five miles through the dense forest in which his\\r\\nwooden hut was situated.', 'Gretchen would have objected.']\n",
      "\n",
      "Total sentences from Edinburgh Ladies' Magazine: 54958\n",
      "Sample: ['Such trials do not injure the body, but they deaden the\\r\\nsoul; they induce a weariness of conflict, and lead into\\r\\nthe reaction of a deliberate refi;sal to feel at all.', 'In a dream once, I found myself choking, stifling in\\r\\ndeep waters; something like great masses of clinging\\r\\nâ\\x96\\xa0weeds imprisoned my feet, so that I could not get out;\\r\\nand the horror of the situation was heiglitened when I\\r\\ndiscovered that this was my own hair, which had all f;illen\\r\\noff.', 'At last 1 struggled on shore, and feebly walked away\\r\\nThe Ladies^ Edinburgh Magazine.', '13\\r\\nfrom the water.', 'After a while, I turned to look at the\\r\\ndepth I had escaped, and behold !']\n"
     ]
    }
   ],
   "source": [
    "attempt_file_sents = dict.fromkeys(attempt_ids)\n",
    "attempt_sents = []\n",
    "# Iterate through each file of a publication of The Attempt\n",
    "for fileid in attempt_ids:\n",
    "    file_sents = sent_tokenize(wordlists.raw(fileid))\n",
    "    attempt_sents += [str(sent) for sent in file_sents]\n",
    "    attempt_file_sents[fileid] = attempt_sents\n",
    "    \n",
    "print(\"Total sentences from The Attempt:\", len(attempt_sents))\n",
    "print(\"Sample:\", attempt_file_sents[\"103655648.txt\"][400:405])\n",
    "print()\n",
    "    \n",
    "mag_file_sents = dict.fromkeys(mag_ids)\n",
    "mag_sents = []\n",
    "# Iterate through each file of a publication of Edinburgh Ladies' Magazine\n",
    "for fileid in mag_ids:\n",
    "    file_sents = sent_tokenize(wordlists.raw(fileid))  \n",
    "    mag_sents += [str(sent) for sent in file_sents]\n",
    "    mag_file_sents[fileid] = mag_sents\n",
    "    \n",
    "print(\"Total sentences from Edinburgh Ladies' Magazine:\", len(mag_sents))\n",
    "print(\"Sample:\", mag_file_sents[\"103655659.txt\"][250:255])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Stemming\n",
    "As we saw in the results of the concordance method, OCR doesn't result in perfectly digitized text.  To get a sense of how many mistakes may have been made in the digitization process, we can measure how many words in the LEDS dataset are recognizable English words according to a list of words used considered valid in the board game [Scrabble](https://raw.githubusercontent.com/jesstess/Scrabble/master/scrabble/sowpods.txt) (as demonstrated in [this example](https://stackoverflow.com/questions/61362891/python-is-there-an-nltk-corpus-for-english-gb-words)).\n",
    "\n",
    "As mentioned in section **1.1 Tokenization**, there are several ways to standardize, or \"normalize,\" text, with each way providing suitable text for different types of analysis.  We're concerned with studying vocabulary, since we want to measure how many of the alphabetic tokens NLTK has identified in the LEDS dataset are valid English words, so we'll work with lowercase, alphabetic tokens from our ``alpha_tokens_lower`` list.\n",
    "\n",
    "To efficiently measure the number of valid and invalid English words, we can further standardize our data through *stemming*.  Stemming reduces words to their root form (i.e. \"abandoning\" becomes \"abandon\").  In the next 3 steps we'll load the Scrabble dataset of valid English words, stem the Scrabble dataset and LEDS dataset, and then see if the stems from the LEDS dataset are present in the Scrabble dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** First we'll load the Scrabble file of words (which helpfully includes British English spellings!) and create a list of those words as a frozen set, which prevents them from being modified accidentally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in Scrabble list: 267752\n",
      "Sample of English words from the Scrabble list: ['abattoirs', 'abattu', 'abature', 'abatures', 'abaxial', 'abaxile', 'abaya', 'abayas', 'abb', 'abba', 'abbacies', 'abbacy', 'abbas', 'abbatial', 'abbe', 'abbed', 'abbes', 'abbess', 'abbesses', 'abbey']\n"
     ]
    }
   ],
   "source": [
    "file = open('data/scrabble_words.txt', 'r')\n",
    "scrabble_words = file.read().split('\\n')\n",
    "scrabble_words_lower = [word.lower() for word in scrabble_words]\n",
    "\n",
    "assert(len(scrabble_words) == len(scrabble_words_lower))  # the number of words shouldn't change when the list is lowercased\n",
    "\n",
    "print(\"Total words in Scrabble list:\", len(scrabble_words))\n",
    "print(\"Sample of English words from the Scrabble list:\", scrabble_words_lower[100:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Next we'll stem the tokens in the Scrabble list and the LEDS dataset.  There are different algorithms that one can use to determine the root of a word; we will use the Porter Stemmer.  To make our code as efficient as possible, we'll create *sets* of the Scrabble and LEDS stems; sets are a Python data structure that are similar to lists, except that each item in a set is unique (there are no duplicates).  \n",
    "\n",
    "This process should give us a smaller number of words to compare and should enable tokens in LEDS to be recognized as English words even if they appeared in a different form in the Scrabble list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['burthen', 'hoydenish', 'herbless', 'deja', 'coiighcd', 'entourag', 'deathâ', 'sabr', 'describ', 'eestonica']\n",
      "['commodif', 'closedown', 'atarax', 'creat', 'antidefam', 'overpackag', 'noncancel', 'cablevis', 'coltwood', 'liturgiologist']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()   # NLTK provides other stemming algorithms you can try, too, such as the LancasterStemmer\n",
    "\n",
    "unique_alpha_lower = list(set(alpha_tokens_lower)) # Remove duplicates from the lowercased, alphabetic tokens in the LEDS dataset\n",
    "leds_porter_stemmed = [porter.stem(t) for t in unique_alpha_lower]\n",
    "\n",
    "scrabble_porter_stemmed = [porter.stem(t) for t in scrabble_words_lower]\n",
    "\n",
    "# Remove duplicates from the Scrabble and LEDS lists of stems\n",
    "leds_pstemmed_set = list(set(leds_porter_stemmed))\n",
    "scrabble_pstemmed_set = list(set(scrabble_porter_stemmed))\n",
    "\n",
    "print(leds_pstemmed_set[:10])\n",
    "print(scrabble_pstemmed_set[50:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:**  Lastly, we'll compare the stems (root forms) of LEDS tokens to the stems of Scrabble words to gauge how many LEDS tokens are recognizable English words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized Stems: 52.320395201129145 %\n"
     ]
    }
   ],
   "source": [
    "recognized_stems = 0\n",
    "for stem in leds_porter_stemmed:\n",
    "    if stem in scrabble_porter_stemmed:\n",
    "        recognized_stems += 1\n",
    "print(\"Recognized Stems:\", (recognized_stems/len(leds_porter_stemmed))*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################\n",
    "# You could also compare each Scrabble word to each alphabetic token in the LEDS dataset; this will take much longer, though\n",
    "#############################################################################################################################\n",
    "# recognized_words = 0\n",
    "# for t in alpha_tokens_lower:\n",
    "#     if t in scrabble_words_lower:\n",
    "#         recognized_words += 1\n",
    "# print(\"Recognized Words:\", (recognized_words/len(alpha_tokens_lower))*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Lemmatization\n",
    "Another form of standardization is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summary Statistics\n",
    "[Code cells in this section will have one function each, preceded with comments in a markdown cell narrating the summarization process]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Frequencies and Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Uniqueness and Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here - most frequent words, sentence structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Analysis (this section will be included for 2-3 datasets)\n",
    "[Code cells in this section will have one function each, preceded with comments in a markdown cell posing an exploratory research question]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 [exploratory research question 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 [exploratory research question 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
