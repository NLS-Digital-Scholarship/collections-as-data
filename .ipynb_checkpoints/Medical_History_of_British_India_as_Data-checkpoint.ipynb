{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Medical History of British India as Data\n",
    "Created in July and August 2020 for the National Library of Scotland's Data Foundry by Lucy Havens, Digital Library Research Intern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the *A Medical History of British India* Dataset\n",
    "[1-2 sentence description of collection and its acquisition]\n",
    "* Data format: digitised text\n",
    "* Data creation process: Optical Character Recognition (OCR) and manual cleaning\n",
    "* Data source: https://data.nls.uk/data/digitised-collections/a-medical-history-of-british-india/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preparation\n",
    "Import libraries to use for cleaning, summarizing and exploring the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lucy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# To prevent SSL certificate failure\n",
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "    getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Libraries for data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Libraries for visualization\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Libraries for text analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.draw.dispersion import dispersion_plot as displt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nls-text-indiaPapers folder (downloadable as *Just the text* data from the website at the top of this notebook) contains TXT files of digitized text, with numerical names, as well as a CSV inventory file and a TXT ReadMe file.  Load only the TXT files of digitized text and **tokenize** the text (which splits a string into separate words and punctuation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No', '.', '1111', '(', 'Sanitary', '),', 'dated', 'Ootacamund', ',', 'the']\n"
     ]
    }
   ],
   "source": [
    "corpus_folder = 'data/nls-text-indiaPapers/'\n",
    "wordlists = PlaintextCorpusReader(corpus_folder, '\\d.*', encoding='latin1')\n",
    "corpus_tokens = wordlists.words()\n",
    "print(corpus_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you'd like to see how to specify a single TXT file to load as data, check out the Jupyter Notebook for the Britain and UK Handbooks!*\n",
    "\n",
    "It's hard to get a sense of how accurately the text has been digitized from this list of 10 tokens, so let's look at one of these words in context.  To see phrases in which \"India\" is used, we can use the concordance() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 16495 matches:\n",
      "ffg . Secretary to the Government of India . Resolution of Government of India \n",
      " India . Resolution of Government of India No . 1 - 137 , dated 5th March 1875 \n",
      "rch 1875 . Letter from Government of India No . 486 , dated 5th September 1876 \n",
      "ember 1876 . Letter to Government of India No . 1063 , dated 26th ditto . REFER\n",
      "ffg . Secretary to the Government of India , Home Department . REFERRING to par\n",
      " to paragraph 8 of the Government of India ' s Resolu - tion No . 1 - 136 , dat\n",
      "inion expressed by the Government of India that any measures of segragation and\n",
      "filth with which all the villages in India are surrounded is quite sufficient t\n",
      "the disease in Rajputana and Central India are in the hands of the Presidency S\n",
      "ffg . Secretary to the Government of India , Home Dept . IN continuation of my \n",
      " the Resolution of the Government of India , Home Department ( Medical ), No 1 \n",
      "d by the orders of the Government of India dated 5th March 1876 . Report on lep\n",
      "e to the orders of the Government of India , contained in paragraph 8 of Resolu\n",
      "ries propounded by the Government of India . 5 . In regard to the extent to whi\n",
      " of leprosy in Europeans resident in India which have come to his knowledge - o\n",
      " case of an European lady , who left India in 1875 and suffered with symptoms o\n",
      "ffg . Secretary to the Government of India , Home Dept . IN continuation of let\n",
      "the information of the Government of India , copy of a letter from the Officiat\n",
      "rnment of the Punjab . Government of India No . 141 , dated 5th March 1875 , pa\n",
      "ffg . Secretary to the Government of India , Home Department . IN continuation \n",
      "the information of the Government of India , a report on the subject of leprosy\n",
      "the information of the Government of India in the Home Department . 2 . Referri\n",
      "he Secretary to the Govern - ment of India in the Home Department for informati\n",
      "o the Secretary to the Government of India , Home Department , for information \n",
      "g Secretary to the Surgeon General , India , Medical Depart - ment , I have the\n"
     ]
    }
   ],
   "source": [
    "t = Text(corpus_tokens)\n",
    "t.concordance('India', lines=20)  # by default NLTK's concordance method displays 25 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *A Medical History of British India* (MHBI) dataset has been digitized and then manually corrected for errors in the digitization process, so we can be pretty confident in the quality of the text for this dataset.\n",
    "\n",
    "Let's find out just how much text and just how many files we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total...\n",
      "  Characters in MHBI Data: 122297870\n",
      "  Words in MHBI Data: 28333479\n",
      "  Sentences in MHBI Data: 1671768\n",
      "  Files in MHBI Data: 468\n"
     ]
    }
   ],
   "source": [
    "def corpusStatistics(plaintext_corpus_read_lists):\n",
    "    total_chars = 0\n",
    "    total_words = 0\n",
    "    total_sents = 0\n",
    "    total_files = 0\n",
    "    for fileid in plaintext_corpus_read_lists.fileids():\n",
    "        total_chars += len(plaintext_corpus_read_lists.raw(fileid))\n",
    "        total_words += len(plaintext_corpus_read_lists.words(fileid))\n",
    "        total_sents += len(plaintext_corpus_read_lists.sents(fileid))\n",
    "        total_files += 1\n",
    "    print(\"Total...\")\n",
    "    print(\"  Characters in MHBI Data:\", total_chars)\n",
    "    print(\"  Words in MHBI Data:\", total_words)\n",
    "    print(\"  Sentences in MHBI Data:\", total_sents)\n",
    "    print(\"  Files in MHBI Data:\", total_files)\n",
    "\n",
    "corpusStatistics(wordlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fileids` are the names of the files in the data's source folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['74457530.txt', '74457800.txt', '74458285.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileids = list(wordlists.fileids())\n",
    "fileids[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the inventory CSV file from the source folder to match the titles of the papers to the corresponding `fileid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74457530.txt</td>\n",
       "      <td>Distribution and causation of leprosy in Briti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74457800.txt</td>\n",
       "      <td>Report of an outbreak of cholera in Suhutwar, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74458285.txt</td>\n",
       "      <td>Report of an investigation into the causes of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74458388.txt</td>\n",
       "      <td>Account of plague administration in the Bombay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74458575.txt</td>\n",
       "      <td>Inquiry into the circumstances attending an ou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         fileid                                              title\n",
       "0  74457530.txt  Distribution and causation of leprosy in Briti...\n",
       "1  74457800.txt  Report of an outbreak of cholera in Suhutwar, ...\n",
       "2  74458285.txt  Report of an investigation into the causes of ...\n",
       "3  74458388.txt  Account of plague administration in the Bombay...\n",
       "4  74458575.txt  Inquiry into the circumstances attending an ou..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/nls-text-indiaPapers/indiaPapers-inventory.csv', header=None, names=['fileid', 'title'])\n",
    "df.head()  # prints the first 5 rows (df.tail() prints the last 5 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Distribution and causation of leprosy in British India 1875 - IP/HA.2',\n",
       " 'Report of an outbreak of cholera in Suhutwar, Bulliah sub-division - IP/30/PI.2',\n",
       " 'Report of an investigation into the causes of the diseases known in Assam as Kála-Azár and Beri-Beri - IP/3/MB.5']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = list(df['title'])\n",
    "titles[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Code cells in this section will have one function each, preceded by comments as markdown above the cell to narrate the cleaning process]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summary Statistics\n",
    "[Code cells in this section will have one function each, preceded with comments in a markdown cell narrating the summarization process]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Uniqueness and Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Narration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here - most frequent words, sentence structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Analysis (this section will be included for 2-3 datasets)\n",
    "[Code cells in this section will have one function each, preceded with comments in a markdown cell posing an exploratory research question]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 [exploratory research question 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 [exploratory research question 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizations go here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
